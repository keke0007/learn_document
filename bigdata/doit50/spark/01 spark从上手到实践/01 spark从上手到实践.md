# 1 sparkç®€ä»‹

sparkæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è¿ç®—å¼•æ“ï¼›

> æœ€æ—©è¯ç”Ÿäºå¤§æ¼‚äº®åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ï¼Œ2010å¼€æºï¼Œåæˆä¸ºapacheé¡¹ç›®

sparkæ”¯æŒè¯»å†™å„ç±»å¸¸è§çš„å­˜å‚¨ç³»ç»Ÿ

sparkæœ‰ä¸°å¯Œçš„ä¸Šå±‚åº“ï¼šå¦‚spark-sqlï¼Œspark-graphxï¼Œspark-streamingï¼Œspark-mllib ç­‰

sparkç›®å‰æœ€ä¸»è¦çš„åº”ç”¨é¢†åŸŸæ˜¯æ‰¹è®¡ç®—ï¼›

sparkç›®å‰æœ€å¸¸è§çš„ä½¿ç”¨æ–¹å¼ï¼Œæ˜¯ä½¿ç”¨spark-sql &#x20;



# 2 sparkåˆ†å¸ƒå¼çš„å®è§‚è®¤è¯†

## 2.1 **ä¸MRçš„ç±»ä¼¼**

* éƒ½æ˜¯åˆ†å¸ƒå¼çš„è¿ç®—å¼•æ“ï¼›

* åœ¨è¿è¡Œæ—¶ï¼Œä¼šæœ‰ApplicationMasteråŠExecutor(ShuffleMapTaskã€ResultTask)ç­‰è¿è¡Œå®ä¾‹ï¼›

* åœ¨æ•°æ®å¤„ç†çš„é“¾æ¡ä¸­ï¼Œä¹Ÿä¼šæœ‰ä¸Šã€ä¸‹æ¸¸æ•°æ®shuffleçš„è¿‡ç¨‹ï¼›

![](images/diagram.png)



## 2.2 **ä¸MRçš„æœ€å¤§ä¸åŒ**

å¦‚æœä¸€ä¸ªæ•°æ®å¤„ç†ä½œä¸šéœ€è¦å¤šæ¬¡shuffleï¼Œæ¯”å¦‚

```sql
select
    a.gender,
    sum(a.salary),
    avg(b.age)
from a join b on a.id = b.id
group by a.gender

-- è¿™ä¸ªsqlé€»è¾‘ï¼Œç”¨åˆ†å¸ƒå¼è¿ç®—æ¥å®ç°æ—¶ï¼Œéœ€è¦ä¸¤æ¬¡shuffle
ç¬¬1æ¬¡shuffleï¼š æŒ‰idå­—æ®µæ¥shuffle
ç¬¬2æ¬¡shuffleï¼š æŒ‰genderå­—æ®µæ¥shuffle
```

* **åœ¨mapreduceä¸­ï¼Œè¿™ä¸ªéœ€æ±‚éœ€è¦ç”¨ä¸¤ä¸ªjobæ¥å®ç°**

![](images/diagram-1.png)





* **è€Œåœ¨sparkä¸­ï¼Œè¿™ä¸ªéœ€è¦åªè¦ä¸€ä¸ªjobæ¥å®ç°**

![](images/diagram-2.png)







# 3 sparkæ ¸å¿ƒæŠ½è±¡\[RDD]

## 3.1 rdd æ ¸å¿ƒæ¦‚å¿µ

**rddï¼š å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼›**

å®ƒå°±æ˜¯ä¸€ä¸ªå¯¹æ•°æ®çš„æŠ½è±¡ï¼›æ‰€è°“å¯¹æ•°æ®çš„æŠ½è±¡ï¼Œå°±æ˜¯å¯¹æ•°æ®çš„æè¿°

è™½ç„¶åä¸ºæ•°æ®é›†ï¼Œä½†å¹¶ä¸æ˜¯çœŸæ­£çš„â€œjavaä¸­çš„é›†åˆâ€ï¼Œrddä¸­å¹¶ä¸å­˜å‚¨æ•°æ®ï¼



**å®ƒæè¿°çš„æœ€é‡è¦ä¿¡æ¯æœ‰ï¼š**

* **åˆ†åŒºåˆ—è¡¨** ï¼š æœ¬RDDæ‰€ä»£è¡¨çš„æ•°æ®é›†ï¼Œæœ‰å“ªäº›åˆ†åŒº

* **æ•°æ®çš„è®¡ç®—æ–¹æ³•ï¼ˆæˆ–äº§ç”Ÿæ–¹æ³•ï¼‰** ï¼š æ•°æ®è®¡ç®—é€»è¾‘

* **ä¾èµ–åˆ—è¡¨ ï¼š&#x20;**&#x672C;RDD ä¾èµ–çš„ çˆ¶RDD (åˆ†åŒºä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼‰

* **åˆ†åŒºçš„ä¼˜é€‰è®¡ç®—ä½ç½®**ï¼ˆpreferred locationsï¼‰

* åˆ†åŒºå™¨ ï¼š çˆ¶RDDè¯¥å¦‚ä½•åˆ†åŒºï¼Œæ¥å¾—åˆ°æœ¬RDDçš„å„åˆ†åŒº  ï¼ˆä½†åˆ†åŒºå™¨æœ¬èº«æ˜¯è®°å½•åœ¨shuffleåçš„å­rddä¸­ï¼‰



![](images/diagram-3.png)









## 3.2 apiè®¾è®¡ç†å¿µ

åŸºäºsparkç¼–ç¨‹ï¼Œç¬¬ä¸€æ­¥å°±æ˜¯æŠŠè¾“å…¥æ•°æ®æ˜ å°„æˆä¸€ä¸ªRDD

ç„¶åï¼Œåœ¨RDDä¸Šæ·»åŠ è®¡ç®—é€»è¾‘å¾—åˆ°ä¸‹ä¸€ä¸ªRDDï¼Œå¹¶å¯ä¸æ–­æŒç»­ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªè®¡ç®—é€»è¾‘çš„é“¾æ¡

æœ€åï¼Œåœ¨æœ€ç»ˆçš„RDDä¸Šï¼Œè§¦å‘ä½œä¸šçš„è¿è¡Œ

![](images/diagram-4.png)

> å¯ä»¥æŠŠrddç±»æ¯”sqlä¸­çš„è§†å›¾viewï¼ˆæˆ–åµŒå¥—å­æŸ¥è¯¢ï¼‰æ¥ç†è§£ï¼šå±‚å±‚åµŒå¥—æˆ–å±‚å±‚é€’è¿›

```scala
val rdd1 = sc.textFile("/wordcount/input")
val rdd2 = rdd1.flatMap("åˆ‡è¯")
val rdd3 = rdd2.map(è¯->(è¯,1))
val rdd4 = rdd3.groupByKey()
val rdd5 = rdd4.mapValues(è¿­ä»£ç»„ä¸­æ‰€æœ‰çš„1è¿›è¡Œç´¯åŠ )
rdd5.saveAsTextFile("/wordcount/output")
```

åœ¨rddä¸Šè°ƒç”¨çš„å„ç§æ–¹æ³•ï¼Œæœ¯è¯­ç§°&#x4E3A;**ï¼šç®—å­**

**ç®—å­åˆ†ä¸¤ç±»ï¼š &#x20;**

* transformationç®—å­ï¼ˆè½¬æ¢ç®—å­ï¼‰ => **è¿”å›å€¼è¿˜æ˜¯RDD ï¼Œåªæ˜¯ç»„è£…æ•°æ®è½¬æ¢ï¼ˆè®¡ç®—ï¼‰é€»è¾‘ï¼Œä¸ä¼šè§¦å‘ä½œä¸šæ‰§è¡Œ**

* actionç®—å­ï¼ˆè¡ŒåŠ¨ç®—å­ï¼‰ ==> è¿”å›å€¼ä¸æ˜¯RDDï¼Œæˆ–æ²¡æœ‰è¿”å›å€¼ï¼› ç”¨äºè§¦å‘æ•´ä¸ªè®¡ç®—é“¾æ¡ï¼ˆä¹Ÿå°±æ˜¯ä½œä¸šï¼‰çš„æ‰§è¡Œ





# 4 è½¬æ¢ç®—å­ç¤ºä¾‹

> æ·»åŠ ä¾èµ–

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_2.12</artifactId>
        <version>3.1.2</version>
    </dependency>
</dependencies>
```



### 1. textFile  & wholeTextFiles

textFile: å°†ä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶æ˜ å°„æˆä¸€ä¸ªrddï¼Œå¾—åˆ°çš„rddä¸­çš„å…ƒç´ çš„æ•°æ®ç±»å‹å°±æ˜¯stringï¼Œä»£è¡¨æ–‡ä»¶ä¸­çš„ä¸€è¡Œ

wholeTextFilesï¼š å°†ä¸€ä¸ªç›®å½•ä¸‹çš„å¤šä¸ªæ–‡ä»¶æ˜ å°„æˆä¸€ä¸ªrddé›†åˆï¼Œå¾—åˆ°çš„rddä¸­çš„å…ƒç´ æ˜¯KVï¼ŒKæ˜¯æ–‡ä»¶åï¼ŒVæ˜¯æ–‡ä»¶ä¸­çš„ä¸€è¡Œæ•°æ®



## 4.1 æ˜ å°„ç±»ç®—å­





### 2. ğŸ€ map  & mapToPair

mapç®—å­çš„åŠŸèƒ½ä¸ºåšæ˜ å°„ï¼Œå³å°†åŸæ¥çš„RDDä¸­**å¯¹åº”çš„**æ¯ä¸€ä¸ªå…ƒç´ ï¼Œåº”ç”¨å¤–éƒ¨ä¼ å…¥çš„å‡½æ•°è¿›è¡Œè¿ç®—ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„RDD

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.map(_ * 2)
```

![](images/diagram-5.png)



### 3. ğŸ€ flatMap  & flatMapToPair

flatMapç®—å­çš„åŠŸèƒ½ä¸ºæ‰å¹³åŒ–æ˜ å°„ï¼Œå³å°†åŸæ¥RDDä¸­å¯¹åº”çš„æ¯ä¸€ä¸ªå…ƒç´ åº”ç”¨å¤–éƒ¨çš„è¿ç®—é€»è¾‘è¿›è¡Œè¿ç®—ï¼Œç„¶åå†å°†è¿”å›çš„æ•°æ®è¿›è¡Œå‹å¹³ï¼Œç±»ä¼¼å…ˆmapï¼Œç„¶åå†flattençš„æ“ä½œï¼Œæœ€åè¿”å›ä¸€ä¸ªæ–°çš„RDD

* **scalaç‰ˆæœ¬**

```scala
// scalaç‰ˆ
val arr = Array(
  "spark hive flink",
  "hive hive flink",
  "hive spark flink",
  "hive spark flink"
)
val rdd1: RDD[String] = sc.makeRDD(arr, 2)
val rdd2: RDD[String] = rdd1.flatMap(_.split(" "))
```

* **javaç‰ˆæœ¬**

```java
// javaç‰ˆ
JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("spark flink hive", "hive spark hadoop", "hadoop spark"));
JavaRDD<String[]> rdd2 = rdd1.map(s -> s.split(","));
```



### 4. ğŸ€ mapPartitions & mapPartitionsToPair

å°†æ•°æ®ä»¥åˆ†åŒºä¸ºçš„å½¢å¼è¿”å›è¿›è¡Œmapæ“ä½œï¼Œä¸€ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªè¿­ä»£å™¨ï¼Œè¯¥æ–¹æ³•å’Œmapæ–¹æ³•ç±»ä¼¼ï¼Œåªä¸è¿‡è¯¥æ–¹æ³•çš„å‚æ•°ç”±RDDä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ å˜æˆäº†RDDä¸­æ¯ä¸€ä¸ªåˆ†åŒºçš„è¿­ä»£å™¨ï¼Œå¦‚æœåœ¨æ˜ å°„çš„è¿‡ç¨‹ä¸­éœ€è¦é¢‘ç¹åˆ›å»ºé¢å¤–çš„å¯¹è±¡ï¼Œä½¿ç”¨mapPartitionsè¦æ¯”mapé«˜æ•ˆçš„è¿‡ã€‚

> mapå’ŒmapPartitionsçš„åŒºåˆ«ï¼ŒmapPartitionsä¸€å®šä¼šæ¯”mapæ•ˆç‡æ›´é«˜å—ï¼Ÿ
>
> ä¸ä¸€å®šï¼šå¦‚æœå¯¹RDDä¸­çš„æ•°æ®è¿›è¡Œç®€å•çš„æ˜ å°„æ“ä½œï¼Œä¾‹å¦‚å˜å¤§å†™ï¼Œå¯¹æ•°æ®è¿›è¡Œç®€å•çš„è¿ç®—ï¼Œmapå’ŒmapPartitionsçš„æ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯å¦‚æœæ˜¯ä½¿ç”¨åˆ°äº†å¤–éƒ¨å…±äº«çš„å¯¹è±¡æˆ–æ•°æ®åº“è¿æ¥ï¼ŒmapPartitionsæ•ˆç‡ä¼šæ›´é«˜ä¸€äº›ã€‚
>
> åŸå› ï¼šmapå‡ºå…¥çš„å‡½æ•°æ˜¯ä¸€æ¡ä¸€æ¡çš„è¿›è¡Œå¤„ç†ï¼Œå¦‚æœä½¿ç”¨æ•°æ®åº“è¿æ¥ï¼Œä¼šæ¯æ¥ä¸€æ¡æ•°æ®åˆ›å»ºä¸€ä¸ªè¿æ¥ï¼Œå¯¼è‡´æ€§èƒ½è¿‡ä½ï¼Œè€ŒmapPartitionsä¼ å…¥çš„å‡½æ•°å‚æ•°æ˜¯è¿­ä»£å™¨ï¼Œæ˜¯ä»¥åˆ†åŒºä¸ºå•ä½è¿›è¡Œæ“ä½œï¼Œå¯ä»¥äº‹å…ˆåˆ›å»ºå¥½ä¸€ä¸ªè¿æ¥ï¼Œåå¤ä½¿ç”¨ï¼Œæ“ä½œä¸€ä¸ªåˆ†åŒºä¸­çš„å¤šæ¡æ•°æ®ã€‚
>
> ç‰¹åˆ«æé†’ï¼šå¦‚æœä½¿ç”¨mapPartitionsæ–¹æ³•ä¸å½“ï¼Œå³å°†è¿­ä»£å™¨ä¸­çš„æ•°æ®toListï¼Œå°±æ˜¯å°†æ•°æ®éƒ½æ”¾åˆ°å†…å­˜ä¸­ï¼Œå¯èƒ½ä¼šå‡ºç°å†…å­˜æº¢å‡ºçš„æƒ…å†µã€‚

![](images/diagram-6.png)

* **scalaç‰ˆæœ¬**

```scala
// scalaç‰ˆ
val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5), 2)
var r1: RDD[Int] = rdd1.mapPartitions(it => it.map(x => x * 10))
```

* **javaç‰ˆæœ¬**

```java
// javaç‰ˆ
JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,6,8),4);
JavaRDD<Integer> integerJavaRDD = rdd1.mapPartitions(iter -> {
    return new Iterator<Integer>() {
        @Override
        public boolean hasNext() {
            return iter.hasNext();
        }

        @Override
        public Integer next() {
            return iter.next()*2;
        }
    };

}); 
```

###



### 5. mapPartitionsWithIndex

ç±»ä¼¼äºmapPartitions, ä¸è¿‡å‡½æ•°è¦è¾“å…¥ä¸¤ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºåˆ†åŒºçš„ç´¢å¼•ï¼Œç¬¬äºŒä¸ªæ˜¯å¯¹åº”åˆ†åŒºçš„è¿­ä»£å™¨ã€‚å‡½æ•°çš„è¿”å›çš„æ˜¯ä¸€ä¸ªç»è¿‡è¯¥å‡½æ•°è½¬æ¢çš„è¿­ä»£å™¨ã€‚

* **scalaç‰ˆæœ¬**

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)
val rdd2 = rdd1.mapPartitionsWithIndex((index, it) => {
  it.map(e => s"partition: $index, val: $e")
})
```

* **javaç‰ˆæœ¬**

```java
JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,6,8),4);

// æ­¤å¤„çš„indexï¼ŒæŒ‡çš„æ˜¯æ•°æ®å…ƒç´ æ‰€å±çš„partition id
JavaRDD<String> res = rdd1.mapPartitionsWithIndex((pidx, iter) -> new Iterator<String>() {

    @Override
    public boolean hasNext() {
        return iter.hasNext();
    }

    @Override
    public String next() {
        return iter.next() + "=>" + pidx;
    }
}, true);
```



### 6. ğŸ€ filter

filterçš„åŠŸèƒ½ä¸ºè¿‡æ»¤ï¼Œå³å°†åŸæ¥RDDä¸­å¯¹åº”çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œåº”ç”¨å¤–éƒ¨ä¼ å…¥çš„è¿‡æ»¤é€»è¾‘ï¼Œç„¶åè¿”å›ä¸€ä¸ªæ–°çš„çš„RDD



* **scalaç‰ˆæœ¬**

```scala
 val rdd1: RDD[Int] = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 2)
 val rdd2: RDD[Int] = rdd1.filter(_ % 2 == 0)
 val rdd2: RDD[Int] = rdd1.filter(x => x % 2 == 0)
```

* **javaç‰ˆæœ¬**

```java
JavaRDD<String> rdd1 = sc.parallelize(Arrays.asList("spark flink hive", "hive spark hadoop", "hadoop spark"));
JavaRDD<String> rdd2 = rdd1.filter(s -> s.startsWith("a"));
```







***



### 7. keys

RDDä¸­çš„æ•°æ®ä¸ºå¯¹å¶å…ƒç»„ç±»å‹ï¼Œè°ƒç”¨keysæ–¹æ³•åè¿”å›ä¸€ä¸ªæ–°çš„çš„RDDï¼Œè¯¥RDDçš„å¯¹åº”çš„æ•°æ®ä¸ºåŸæ¥å¯¹å¶å…ƒç»„çš„å…¨éƒ¨keyï¼Œè¯¥æ–¹æ³•æœ‰éšå¼è½¬æ¢

* **scalaç‰ˆæœ¬**

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val keyRDD: RDD[String] = wordAndOne.keys
```

* **javaç‰ˆæœ¬**

```java
JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,6,8),4);

JavaPairRDD<Integer, Integer> pairRdd = rdd1.mapToPair(i -> Tuple2.apply(i, i * 2));
JavaRDD<Integer> res = pairRdd.keys();

System.out.println(res.collect());  // [1, 2, 3, 6, 8]
```





### 8. values

RDDä¸­çš„æ•°æ®ä¸ºå¯¹å¶å…ƒç»„ç±»å‹ï¼Œè°ƒç”¨valuesæ–¹æ³•åè¿”å›ä¸€ä¸ªæ–°çš„çš„RDDï¼Œè¯¥RDDçš„å¯¹åº”çš„æ•°æ®ä¸ºåŸæ¥å¯¹å¶å…ƒç»„çš„å…¨éƒ¨values



* **scalaç‰ˆæœ¬**

```scala
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val valueRDD: RDD[Int] = wordAndOne.values
```

* **javaç‰ˆæœ¬**

```java
JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,6,8),4);

JavaPairRDD<Integer, Integer> pairRdd = rdd1.mapToPair(i -> Tuple2.apply(i, i * 2));
JavaRDD<Integer> res = pairRdd.values();

System.out.println(res.collect());   // [2, 4, 6, 12, 16]
```





### 9. mapValues

RDDä¸­çš„æ•°æ®ä¸ºå¯¹å¶å…ƒç»„ç±»å‹ï¼Œå°†valueåº”ç”¨ä¼ å…¥çš„å‡½æ•°è¿›è¡Œè¿ç®—åå†ä¸keyç»„åˆæˆå…ƒç»„è¿”å›ä¸€ä¸ªæ–°çš„RDD



* **scalaç‰ˆæœ¬**

```scala
val lst = List(("spark", 5), ("hive", 3), ("hbase", 4), ("flink", 8))
val rdd1: RDD[(String, Int)] = sc.parallelize(lst, 2)
//å°†æ¯ä¸€ä¸ªå…ƒç´ çš„æ¬¡æ•°ä¹˜ä»¥10å†å¯è·Ÿkeyç»„åˆåœ¨ä¸€èµ·
//val rdd2 = rdd1.map(t => (t._1, t._2 * 10))
val rdd2 = rdd1.mapValues(_ * 10)
```



* **javaç‰ˆæœ¬**

```java
JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1,2,3,6,8),4);

JavaPairRDD<Integer, Integer> pairRdd = rdd1.mapToPair(i -> Tuple2.apply(i, i * 2));
// è¿”å›æ­¤å‰çš„keyå’Œå¤„ç†åçš„value
JavaPairRDD<Integer, Integer> res = pairRdd.mapValues(v -> Math.min(v, 6));

System.out.println(res.collect());  // [(1,2), (2,4), (3,6), (6,6), (8,6)]
```





### 10. flatMapValues

RDDä¸­çš„æ•°æ®ä¸ºå¯¹å¶å…ƒç»„ç±»å‹ï¼Œå°†valueåº”ç”¨ä¼ å…¥çš„å‡½æ•°è¿›è¡ŒflatMapæ‰“å¹³åå†ä¸keyç»„åˆæˆå…ƒç»„è¿”å›ä¸€ä¸ªæ–°çš„RDD

* **scalaç‰ˆæœ¬**

```scala
/** scala ç‰ˆæœ¬ **/
val lst = List(("spark", "1,2,3"), ("hive", "4,5"), ("hbase", "6"), ("flink", "7,8"))
val rdd1: RDD[(String, String)] = sc.parallelize(lst, 2)
//å°†valueæ‰“å¹³ï¼Œå†å°†æ‰“å¹³åçš„æ¯ä¸€ä¸ªå…ƒç´ ä¸keyç»„åˆ("spark", "1,2,3") =>ï¼ˆ"spark",1ï¼‰,ï¼ˆ"spark",2ï¼‰,ï¼ˆ"spark",3ï¼‰
val rdd2: RDD[(String, Int)] = rdd1.flatMapValues(_.split(",").map(_.toInt))
//    val rdd2 = rdd1.flatMap(t => {
//      t._2.split(",").map(e => (t._1, e.toInt))
//    })
```

* **javaç‰ˆæœ¬**

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, String>> list = Arrays.asList(
        Tuple2.apply("spark", "1,2,3"),
        Tuple2.apply("hive", "4,5"),
        Tuple2.apply("hbase", "6"),
        Tuple2.apply("flink", "7,8"));

JavaRDD<Tuple2<String, String>> rdd1 = sc.parallelize(list);
JavaPairRDD<String, String> pairRDD = rdd1.mapToPair(p -> p);

JavaPairRDD<String, Integer> res = pairRDD.flatMapValues(new FlatMapFunction<String, Integer>() {
    @Override
    public Iterator<Integer> call(String s) throws Exception {
        return Arrays.stream(s.split(",")).mapToInt(Integer::parseInt).iterator();
    }
});

// [(spark,1), (spark,2), (spark,3), (hive,4), (hive,5), (hbase,6), (flink,7), (flink,8)]
System.out.println(res.collect());
```



## 4.2 èšåˆç±»ç®—å­



### 11. ğŸ€ aggregateByKey

![](images/diagram-7.png)

ä¸reduceByKeyç±»ä¼¼ï¼Œå¹¶ä¸”å¯ä»¥æŒ‡å®šåˆå§‹å€¼ï¼Œæ¯ä¸ªåˆ†åŒºåº”ç”¨ä¸€æ¬¡åˆå§‹å€¼ï¼Œä¼ å…¥ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«æ˜¯å±€éƒ¨èšåˆçš„è®¡ç®—é€»è¾‘ã€å…¨å±€èšåˆçš„é€»è¾‘ã€‚

* **scalaç‰ˆæœ¬**

```scala
/** scala ç‰ˆæœ¬ **/
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//åœ¨ç¬¬ä¸€ä¸ªæ‹¬å·ä¸­ä¼ å…¥åˆå§‹åŒ–ï¼Œç¬¬äºŒä¸ªæ‹¬å·ä¸­ä¼ å…¥ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«æ˜¯å±€éƒ¨èšåˆçš„é€»è¾‘å’Œå…¨å±€èšåˆçš„é€»è¾‘
val reduced: RDD[(String, Int)] = wordAndOne.aggregateByKey(0)(_ + _, _ + _)
```

* **javaç‰ˆæœ¬**

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, Integer>> datas = Arrays.asList(
        Tuple2.apply("spark", 1), Tuple2.apply("hive", 1), Tuple2.apply("hive", 1), Tuple2.apply("spark", 1),
        Tuple2.apply("spark", 1), Tuple2.apply("hbase", 1), Tuple2.apply("hbase", 1), Tuple2.apply("spark", 1));
JavaPairRDD<String, Integer> pairRdd = sc.parallelizePairs(datas);

// æŒ‡å®šä¸€ä¸ªèšåˆçš„åˆå§‹å€¼(èšåˆå€¼å¯ä»¥æ˜¯ä»»æ„ç±»å‹ï¼‰
pairRdd.aggregateByKey(
        new ArrayList<String>(),  // é›¶å€¼çš„åˆå§‹ç´¯åŠ å™¨
        (lst,v)->{lst.add(String.valueOf(v));return lst;},  // èšåˆå…ƒç´ åˆ°ç´¯åŠ å™¨çš„å‡½æ•°
        (l1,l2)->{l1.addAll(l2);return l1;}  // èšåˆç´¯åŠ å™¨åˆ°ç´¯åŠ å™¨çš„å‡½æ•°
);
```



### 12. ğŸ€ reduceByKey

å°†æ•°æ®æŒ‰ç…§ç›¸åŒçš„keyè¿›è¡Œèšåˆï¼Œç‰¹ç‚¹æ˜¯å…ˆåœ¨æ¯ä¸ªåˆ†åŒºä¸­è¿›è¡Œå±€éƒ¨åˆ†ç»„èšåˆï¼Œç„¶åå°†æ¯ä¸ªåˆ†åŒºèšåˆçš„ç»“æœä»ä¸Šæ¸¸æ‹‰å–åˆ°ä¸‹æ¸¸å†è¿›è¡Œå…¨å±€åˆ†ç»„èšåˆ

* **scalaç‰ˆæœ¬**

```scala
/** scala ç‰ˆæœ¬ **/
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
```

* **javaç‰ˆæœ¬**

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, Integer>> datas = Arrays.asList(
        Tuple2.apply("spark", 1), Tuple2.apply("hadoop", 1), Tuple2.apply("hive", 1), Tuple2.apply("spark", 1),
        Tuple2.apply("spark", 1), Tuple2.apply("flink", 1), Tuple2.apply("hbase", 1), Tuple2.apply("spark", 1));

JavaPairRDD<String, Integer> pairRdd = sc.parallelizePairs(datas);
JavaPairRDD<String, Integer> res = pairRdd.reduceByKey(Integer::sum);  //(v1,v2)->v1+v2
System.out.println(res.collect());
```

###

### 13. foldByKey

ä¸reduceByKeyç±»ä¼¼ï¼Œåªä¸è¿‡æ˜¯å¯ä»¥æŒ‡å®šåˆå§‹å€¼ï¼Œæ¯ä¸ªåˆ†åŒºåº”ç”¨ä¸€æ¬¡åˆå§‹å€¼ï¼Œå…ˆåœ¨æ¯ä¸ªè¿›è¡Œå±€éƒ¨èšåˆï¼Œç„¶åå†å…¨å±€èšåˆï¼Œå±€éƒ¨èšåˆçš„é€»è¾‘ä¸å…¨å±€èšåˆçš„é€»è¾‘ç›¸åŒã€‚

* **scalaç‰ˆæœ¬**

```scala
 /** scala ç‰ˆæœ¬ **/
 val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)

//ä¸reduceByKeyç±»ä¼¼ï¼Œåªä¸è¿‡æ˜¯å¯ä»¥æŒ‡å®šåˆå§‹å€¼ï¼Œæ¯ä¸ªåˆ†åŒºåº”ç”¨ä¸€æ¬¡åˆå§‹å€¼
val reduced: RDD[(String, Int)] = wordAndOne.foldByKey(0)(_ + _)
```

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, Integer>> datas = Arrays.asList(
        Tuple2.apply("spark", 1), Tuple2.apply("hive", 1), Tuple2.apply("hive", 1), Tuple2.apply("spark", 1),
        Tuple2.apply("spark", 1), Tuple2.apply("hbase", 1), Tuple2.apply("hbase", 1), Tuple2.apply("spark", 1));
JavaPairRDD<String, Integer> pairRdd = sc.parallelizePairs(datas);

// æŒ‡å®šä¸€ä¸ªèšåˆçš„åˆå§‹å€¼
JavaPairRDD<String, Integer> res = pairRdd.foldByKey(10, Integer::sum);
// [(hive,22), (spark,44), (hbase,22)]
System.out.println(res.collect());
```





### 14. combineByKey

> combineByKey  ä¸  aggregateBykey å’Œ reduceByKeyçš„ä¸€ä¸ªç»†èŠ‚åŒºåˆ«ï¼š &#x20;
>
> combineByKey  å¯ä»¥åœ¨è°ƒç”¨æ—¶**æŒ‡å®šæ˜¯å¦åšmapç«¯é¢„èšåˆ**
>
> è€Œå¦å¤–ä¸¤ä¸ªæ˜¯å†™æ­»çš„ï¼Œä¸€å®šä¼šåšmapç«¯é¢„èšåˆ

* **scalaç‰ˆæœ¬**

```scala
/** scala ç‰ˆæœ¬ **/
val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//è°ƒç”¨combineByKeyä¼ å…¥ä¸‰ä¸ªå‡½æ•°
//val reduced = wordAndOne.combineByKey(x => x, (a: Int, b: Int) => a + b, (m: Int, n: Int) => m + n)
val f1 = (x: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f1 function invoked in state: $stage, partition: $partition")
  x
}
//åœ¨æ¯ä¸ªåˆ†åŒºå†…ï¼Œå°†keyç›¸åŒçš„valueè¿›è¡Œå±€éƒ¨èšåˆæ“ä½œ
val f2 = (a: Int, b: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f2 function invoked in state: $stage, partition: $partition")
  a + b
}
//ç¬¬ä¸‰ä¸ªå‡½æ•°æ˜¯åœ¨ä¸‹æ¸¸å®Œæˆçš„
val f3 = (m: Int, n: Int) => {
  val stage = TaskContext.get().stageId()
  val partition = TaskContext.getPartitionId()
  println(s"f3 function invoked in state: $stage, partition: $partition")
  m + n
}
val reduced = wordAndOne.combineByKey(f1, f2, f3)
```

* **javaç‰ˆæœ¬**

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, Integer>> datas = Arrays.asList(
            Tuple2.apply("spark", 1), Tuple2.apply("hive", 1), Tuple2.apply("hive", 1), Tuple2.apply("spark", 1),
            Tuple2.apply("spark", 1), Tuple2.apply("hbase", 1), Tuple2.apply("hbase", 1), Tuple2.apply("spark", 1));
JavaPairRDD<String, Integer> pairRdd = sc.parallelizePairs(datas);


    // ä»¥ä¸€ä¸ªæ•´æ•°ä½œä¸ºç´¯åŠ å™¨
    JavaPairRDD<String, Integer> res = pairRdd.combineByKey(i -> 1, Integer::sum, Integer::sum);

    // ä»¥ä¸€ä¸ªè‡ªå®šä¹‰ç±»å‹ä½œä¸ºç´¯åŠ å™¨
    JavaPairRDD<String, Agg> res = pairRdd.combineByKey(
            // ç”Ÿæˆä¸€ä¸ªåŒ…å«ç¬¬ä¸€æ¡æ•°æ®çš„ç´¯åŠ å™¨ï¼ˆä¸æ˜¯ä¸€ä¸ªé›¶å€¼ç´¯åŠ å™¨ï¼‰
            v -> {
                Agg agg = new Agg();
                agg.lst.add(v);
                return agg;
            },
            // å±€éƒ¨èšåˆï¼ˆå¯èƒ½å‘ç”Ÿä¹Ÿå¯èƒ½ä¸å‘ç”Ÿï¼‰
            (agg, v) -> {
                agg.lst.add(v);
                return agg;
            }
            // å…¨å±€èšåˆ
            , (agg1, agg2) -> {
                agg1.lst.addAll(agg2.lst);
                return agg1;
            });

    System.out.println(res.collect());
}


public static class Agg implements Serializable {

    List<Integer> lst = new ArrayList<>();

    @Override
    public String toString() {
        return lst.toString();
    }
}
```





###

### 15. groupByKey  & groupBy

æŒ‰ç…§keyè¿›è¡Œåˆ†ç»„(ä»…åˆ†ç»„ä¸èšåˆ)

* **scalaç‰ˆæœ¬**

```scala
  /** scala ç‰ˆæœ¬ **/
 val lst = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
//æŒ‰ç…§keyè¿›è¡Œåˆ†ç»„
val grouped: RDD[(String, Iterable[Int])] = wordAndOne.groupByKey()
```

```java
/** java ç‰ˆæœ¬ **/
List<Tuple2<String, Integer>> datas = Arrays.asList(
        Tuple2.apply("spark", 1), Tuple2.apply("hive", 1), Tuple2.apply("hive", 1), Tuple2.apply("spark", 1),
        Tuple2.apply("spark", 1), Tuple2.apply("hbase", 1), Tuple2.apply("hbase", 1), Tuple2.apply("spark", 1));
JavaPairRDD<String, Integer> pairRdd = sc.parallelizePairs(datas);

// è¿”å›çš„rddæ˜¯ <key,valueè¿­ä»£å™¨>
JavaPairRDD<String, Iterable<Integer>> grouped = pairRdd.groupByKey();
```





### 16. distinct å»é‡

distinctæ˜¯å¯¹RDDä¸­çš„å…ƒç´ è¿›è¡Œå–é‡ï¼Œåº•å±‚ä½¿ç”¨çš„æ˜¯reduceByKeyå®ç°çš„ï¼Œå…ˆå±€éƒ¨å»é‡ï¼Œç„¶åå†å…¨å±€å»é‡

* **scalaç‰ˆæœ¬**

```scala
val arr = Array(
  "spark", "hive", "spark", "flink",
  "spark", "hive", "hive", "flink",
  "flink", "flink", "flink", "spark"
)
val rdd1: RDD[String] = sc.parallelize(arr, 3)
//å»é‡
val rdd2: RDD[String] = rdd1.distinct()
```

distinctçš„åº•å±‚å®ç°å¦‚ä¸‹ï¼š

```scala
val rdd11: RDD[(String, Null)] = rdd1.map((_, null))
val rdd12: RDD[String] = rdd11.reduceByKey((a, _) => a).keys
```



* **javaç‰ˆæœ¬**

> å¯¹è‡ªå®šä¹‰ç±»å‹å»é‡ï¼Œéœ€è¦é‡å†™hashcode

```java
    JavaSparkContext sc = new JavaSparkContext(conf);
    
    // å¯¹åŸºæœ¬ç±»å‹å»é‡
    JavaRDD<Integer> rdd1 = sc.parallelize(Arrays.asList(1, 3, 5, 1, 3, 3, 6));
    JavaRDD<Integer> distinct1 = rdd1.distinct();



    // å¯¹è‡ªå®šä¹‰ç±»å‹å»é‡
    // éœ€è¦é‡å†™hashcodeï¼šè®©å¯¹è±¡æˆå‘˜å€¼ç›¸åŒçš„è¿”å›ç›¸åŒçš„hashcode
    List<Person> lst = Arrays.asList(
            new Person("aa", 18),
            new Person("bb", 28),
            new Person("cc", 38),
            new Person("aa", 18),
            new Person("bb", 28),
            new Person("dd", 38)
    );
    JavaRDD<Person> rdd = sc.parallelize(lst);
    JavaRDD<Person> distinct = rdd.distinct();

    distinct.foreach(e-> System.out.println(e));

}

@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
@ToString
@Data  // å°è¾£æ¤’ç”Ÿæˆçš„hashcodeï¼Œæ˜¯åˆ©ç”¨å¯¹è±¡ä¸­æˆå‘˜å˜é‡å€¼ç”Ÿæˆï¼›å¦‚æœæˆå‘˜å˜é‡å€¼ç›¸åŒï¼Œåˆ™ä¸¤å¯¹è±¡çš„hashcodeå°±ç›¸åŒ
public static class Person implements Serializable {
    private String name;
    private int age;
}
```

> å»é‡çš„å®ç°ï¼š
>
> å…ˆæŠŠrdd\[K] => rdd\[K,v] => reduceBykey() => è¿”å›key







### 17. sortBy  & sortByKey

* sortByé’ˆå¯¹ä»»æ„RDDï¼Œéœ€è¦ä¼ å…¥ä¸€ä¸ªæŠ½å–keyçš„function

* sortBykeyé’ˆå¯¹KV RDD

æŒ‰ç…§æŒ‡çš„çš„æ’åºè§„åˆ™è¿›è¡Œå…¨å±€æ’åº

* **scalaç‰ˆæœ¬ sortBy**

```scala
val lines: RDD[String] = sc.textFile("hdfs://node-1.51doit.cn:9000/words")
//åˆ‡åˆ†å‹å¹³
val words: RDD[String] = lines.flatMap(_.split(" "))
//å°†å•è¯å’Œ1ç»„åˆ
val wordAndOne: RDD[(String, Int)] = words.map((_, 1))
//åˆ†ç»„èšåˆ
val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
//æŒ‰ç…§å•è¯å‡ºç°çš„æ¬¡æ•°ï¼Œä»é«˜åˆ°ä½è¿›è¡Œæ’åº
val sorted: RDD[(String, Int)] = reduced.sortBy(_._2, false)
```

* **scalaç‰ˆæœ¬ sortByKey**

```scala
val lines: RDD[String] = sc.textFile("hdfs://node-1.51doit.cn:9000/words")
//åˆ‡åˆ†å‹å¹³
val words: RDD[String] = lines.flatMap(_.split(" "))
//å°†å•è¯å’Œ1ç»„åˆ
val wordAndOne: RDD[(String, Int)] = words.map((_, 1))
//åˆ†ç»„èšåˆ
val reduced: RDD[(String, Int)] = wordAndOne.reduceByKey(_ + _)
//æŒ‰ç…§å•è¯å‡ºç°çš„æ¬¡æ•°ï¼Œä»é«˜åˆ°ä½è¿›è¡Œæ’åº
//val sorted: RDD[(String, Int)] = reduced.sortBy(_._2, false)
//val keyed: RDD[(Int, (String, Int))] = reduced.keyBy(_._2).sortByKey()
val sorted = reduced.map(t => (t._2, t)).sortByKey(false)
```

> sortByã€sortByKeyæ˜¯Transformationï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆä¼šç”Ÿæˆjobï¼Ÿ
>
> å› ä¸ºsortByã€sortByKeyéœ€è¦å®ç°å…¨å±€æ’åºï¼Œä½¿ç”¨çš„æ˜¯**RangePartitioner**ï¼Œåœ¨æ„å»ºRangePartitioneræ—¶ï¼Œä¼šå¯¹æ•°æ®è¿›è¡Œé‡‡æ ·æ¥ç”ŸæˆrangePartitioneréœ€è¦çš„åˆ†ç•Œç‚¹ï¼Œæ‰€ä»¥ä¼šè§¦å‘Actionï¼Œæ ¹æ®é‡‡æ ·çš„ç»“æœæ¥æ„å»ºRangePartitionerã€‚
>
> RangePartitionerå¯ä»¥ä¿è¯æ•°æ®æŒ‰ç…§ä¸€å®šçš„èŒƒå›´å…¨å±€æœ‰åºï¼ŒåŒæ—¶åœ¨shuffleçš„åŒæ—¶ï¼Œæœ‰è®¾ç½®äº†setKeyOrderingæŒ‡å®šäº†æ’åºè§„åˆ™ï¼Œè¿™æ ·å°±åˆå¯ä»¥ä¿è¯æ•°æ®åœ¨æ¯ä¸ªåˆ†åŒºå†…æœ‰åºäº†ï¼



###

### 18. repartitionAndSortWithinPartitions

> æ¯”èµ·æˆ‘ä»¬æŠŠæ•°æ®åˆ†ç»„ä¹‹åï¼Œè‡ªå·±ç”¨è¿­ä»£å™¨è·å–åˆ°ä¸€ç»„æ•°æ®æ”¾å…¥å†…å­˜æ’åºï¼Œæ•ˆç‡æ›´é«˜
>
> **å› ä¸ºï¼Œè¿™ä¸ªç®—å­çš„åº•å±‚ä¼šæŠŠæ’åºçš„é€»è¾‘ï¼Œä¸‹æ¨åˆ°shuffleçš„æœºåˆ¶ä¸­å»**



æŒ‰ç…§å€¼çš„åˆ†åŒºå™¨è¿›è¡Œåˆ†åŒºï¼Œå¹¶ä¸”å°†æ•°æ®æŒ‰ç…§æŒ‡çš„çš„æ’åºè§„åˆ™åœ¨**åˆ†åŒºå†…æ’åº**ï¼Œåº•å±‚ä½¿ç”¨çš„æ˜¯ShuffledRDDï¼Œè®¾ç½®äº†æŒ‡å®šçš„åˆ†åŒºå™¨å’Œæ’åºè§„åˆ™

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 3), ("hadoop", 1), ("hive",3), ("spark", 2),
  ("spark", 9), ("flink", 2), ("hbase", 1), ("spark", 4),
  ("kafka", 8), ("kafka", 5), ("kafka", 7), ("kafka", 1),
  ("hadoop", 5), ("flink", 4), ("hive", 6), ("flink", 3)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val partitioner = new HashPartitioner(wordAndOne.partitions.length)
//æŒ‰ç…§æŒ‡å®šçš„åˆ†åŒºè¿›è¡Œåˆ†åŒºï¼Œå¹¶ä¸”å°†æ•°æ®æŒ‰ç…§æŒ‡å®šçš„æ’åºè§„åˆ™åœ¨åˆ†åŒºå†…æ’åº
val partitioned = wordAndOne.repartitionAndSortWithinPartitions(partitioner)
```

repartitionAndSortWithinPartitionsçš„åº•å±‚å®ç°ï¼š

```scala
new ShuffledRDD[K, V, V](self, partitioner).setKeyOrdering(ordering)
```



***









## 4.3 å¤šRDDç®—å­

### 19.  cogroup

ååŒåˆ†ç»„ï¼Œå³å°†å¤šä¸ªRDDä¸­å¯¹åº”çš„æ•°æ®ï¼Œä½¿ç”¨ç›¸åŒçš„åˆ†åŒºå™¨ï¼ˆHashPartitionerï¼‰ï¼Œå°†æ¥è‡ªå¤šä¸ªRDDä¸­çš„keyç›¸åŒçš„æ•°æ®é€šè¿‡ç½‘ç»œä¼ å…¥åˆ°åŒä¸€å°æœºå™¨çš„åŒä¸€ä¸ªåˆ†åŒºä¸­(ä¸*groupByKeyã€groupByåŒºåˆ«æ˜¯ï¼šgroupByKeyã€groupByåªèƒ½å¯¹ä¸€ä¸ªRDDè¿›è¡Œåˆ†ç»„*)

æ³¨æ„:è°ƒç”¨cogroupæ–¹æ³•ï¼Œä¸¤ä¸ªRDDä¸­å¯¹åº”çš„æ•°æ®éƒ½å¿…é¡»æ˜¯å¯¹å¶å…ƒç»„ç±»å‹ï¼Œå¹¶ä¸”keyç±»å‹ä¸€å®šç›¸åŒ

&#x20;

```scala
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªRDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2), ("jerry", 4)), 3)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼å†åˆ›å»ºä¸€ä¸ªRDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2), ("jerry", 4)), 2)
//å°†ä¸¤ä¸ªRDDéƒ½è¿›è¡Œåˆ†ç»„
val grouped: RDD[(String, (Iterable[Int], Iterable[Int]))] = rdd1.cogroup(rdd2)
```

&#x20;

&#x20;

### 20. join

ä¸¤ä¸ªRDDè¿›è¡Œjoinï¼Œç›¸å½“äºSQLä¸­çš„å†…å…³è”join

ä¸¤ä¸ªRDDä¸ºä»€ä¹ˆè¦è¿›è¡Œjoinï¼Ÿæƒ³è¦çš„æ•°æ®æ¥è‡ªäºä¸¤ä¸ªæ•°æ®é›†ï¼Œå¹¶ä¸”ä¸¤ä¸ªæ•°æ®é›†çš„æ•°æ®å­˜åœ¨ç›¸åŒçš„æ¡ä»¶ï¼Œå¿…é¡»å…³è”èµ·æ¥æ‰èƒ½å¾—åˆ°æƒ³è¦çš„å…¨éƒ¨æ•°æ®

```scala
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªRDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)), 2)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼å†åˆ›å»ºä¸€ä¸ªRDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1.0), ("shuke", 2), ("jerry", 4)), 3)
val rdd3: RDD[(String, (Int, Double))] = rdd1.join(rdd2)
```

&#x20;



### 21. leftOuterJoin

å·¦å¤–è¿æ¥ï¼Œç›¸å½“äºSQLä¸­çš„å·¦å¤–å…³è”

```scala
 //é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªRDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)), 2)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼å†åˆ›å»ºä¸€ä¸ªRDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2), ("jerry", 4)), 2)
val rdd3: RDD[(String, (Int, Option[Int]))] = rdd1.leftOuterJoin(rdd2)
```







### 22. rightOuterJoin

å³å¤–è¿æ¥ï¼Œç›¸å½“äºSQLä¸­çš„å³å¤–å…³è”

```scala
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªRDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)), 2)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼å†åˆ›å»ºä¸€ä¸ªRDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2), ("jerry", 4)), 2)
val rdd3: RDD[(String, (Option[Int], Int))] = rdd1.rightOuterJoin(rdd2)
```



### 23. fullOuterJoin

å…¨è¿æ¥ï¼Œç›¸å½“äºSQLä¸­çš„å…¨å…³è”



```scala
 //é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªRDD
val rdd1 = sc.parallelize(List(("tom", 1), ("tom", 2), ("jerry", 3), ("kitty", 2)), 2)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼å†åˆ›å»ºä¸€ä¸ªRDD
val rdd2 = sc.parallelize(List(("jerry", 2), ("tom", 1), ("shuke", 2), ("jerry", 4)), 2)
val rdd3: RDD[(String, (Option[Int], Option[Int]))] = rdd1.fullOuterJoin(rdd2)
```









### 24. union&#x20;

å°†ä¸¤ä¸ªç±»å‹ä¸€æ ·çš„RDDåˆå¹¶åˆ°ä¸€èµ·ï¼Œè¿”å›ä¸€ä¸ªæ–°çš„RDDï¼Œæ–°çš„RDDçš„åˆ†åŒºæ•°é‡æ˜¯åŸæ¥ä¸¤ä¸ªRDDçš„åˆ†åŒºæ•°é‡ä¹‹å’Œï¼Œæ•°æ®æ²¡æœ‰è¢«æ‰“æ•£é‡ç»„ï¼Œå³æ²¡æœ‰shuffle

```scala
//ä¸¤ä¸ªRDDè¿›è¡Œunionï¼Œå¯¹åº”çš„æ•°æ®ç±»å‹å¿…é¡»ä¸€æ ·
//Unionä¸ä¼šå»é‡
val rdd1 = sc.parallelize(List(1,2,3,4), 2)
val rdd2 = sc.parallelize(List(5, 6, 7, 8, 9,10), 3)
val rdd3 = rdd1.union(rdd2)
println(rdd3.partitions.length)
```

```java
List<Tuple2<String, String>> lst1 = Arrays.asList(
        Tuple2.apply("spark", "1,2,3"),
        Tuple2.apply("hive", "4,5")
        );
JavaRDD<Tuple2<String, String>> rdd1 = sc.parallelize(lst1);

List<Tuple2<String, String>> lst2 = Arrays.asList(
        Tuple2.apply("hbase", "6"),
        Tuple2.apply("flink", "7,8"));
JavaRDD<Tuple2<String, String>> rdd2 = sc.parallelize(lst2);

rdd1.union(rdd2).foreach(s->System.out.println(s));
```





### 25. intersection

æ±‚äº¤é›†ï¼Œåº•å±‚ä½¿ç”¨çš„æ˜¯cogroupå®ç°çš„

**äº¤é›†çš„ç»“æœæ˜¯å»é‡çš„**

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,4,6), 2)
val rdd2 = sc.parallelize(List(3,4,5,6,7,8), 2)
//æ±‚äº¤é›†
val rdd3: RDD[Int] = rdd1.intersection(rdd2)

//ä½¿ç”¨cogroupå®ç°intersectionçš„åŠŸèƒ½
val rdd11 = rdd1.map((_, null))
val rdd22 = rdd2.map((_, null))
val rdd33: RDD[(Int, (Iterable[Null], Iterable[Null]))] = rdd11.cogroup(rdd22)
val rdd44: RDD[Int] = rdd33.filter { case (_, (it1, it2)) => it1.nonEmpty && it2.nonEmpty }.keys
```



### 26. subtract

æ±‚ä¸¤ä¸ªRDDçš„å·®é›†ï¼Œå°†ç¬¬ä¸€ä¸ªRDDä¸­çš„æ•°æ®ï¼Œå¦‚æœåœ¨ç¬¬äºŒä¸ªRDDä¸­å‡ºç°äº†ï¼Œå°±ä»ç¬¬ä¸€ä¸ªRDDä¸­ç§»é™¤

**å·®é›†çš„ç»“æœä¸ä¼šåšå»é‡**

```scala
val rdd1 = sc.parallelize(List("A", "B", "C", "D", "E"))
val rdd2 = sc.parallelize(List("A", "B"))

val rdd3: RDD[String] = rdd1.subtract(rdd2)
//è¿”å› C D E
```









### 27. cartesian

ç¬›å¡å°”ç§¯

```scala
val rdd1 = sc.parallelize(List("tom", "jerry"), 2)
val rdd2 = sc.parallelize(List("tom", "kitty", "shuke"), 3)
val rdd3 = rdd1.cartesian(rdd2)
```





## åˆ†åŒºç®—å­

### 28. partitionBy

æŒ‰ç…§æŒ‡å®šçš„åˆ†åŒºå™¨è¿›è¡Œåˆ†åŒºï¼Œåº•å±‚ä½¿ç”¨çš„æ˜¯ShuffledRDD

* **scalaç‰ˆæœ¬**

```scala
val lst: Seq[(String, Int)] = List(
  ("spark", 1), ("hadoop", 1), ("hive", 1), ("spark", 1),
  ("spark", 1), ("flink", 1), ("hbase", 1), ("spark", 1),
  ("kafka", 1), ("kafka", 1), ("kafka", 1), ("kafka", 1),
  ("hadoop", 1), ("flink", 1), ("hive", 1), ("flink", 1)
)
//é€šè¿‡å¹¶è¡ŒåŒ–çš„æ–¹å¼åˆ›å»ºRDDï¼Œåˆ†åŒºæ•°é‡ä¸º4
val wordAndOne: RDD[(String, Int)] = sc.parallelize(lst, 4)
val partitioner = new HashPartitioner(wordAndOne.partitions.length)
//æŒ‰ç…§æŒ‡å®šçš„åˆ†åŒºè¿›è¡Œåˆ†åŒº
val partitioned: RDD[(String, Int)] = wordAndOne.partitionBy(partitioner)
```

* **javaç‰ˆæœ¬**

> partitionByåªèƒ½åœ¨kvRddä¸Šè°ƒç”¨

```java
JavaPairRDD<String, Integer> kvRdd = rdd1.mapToPair(i -> Tuple2.apply(i + "", i));
JavaPairRDD<String, Integer> partitionByRdd = kvRdd.partitionBy(new HashPartitioner(2) {
    @Override
    public int getPartition(Object key) {
        String intStr = (String) key;
        int k = Integer.parseInt(intStr);

        if (k % 2 == 0) {
            return 1;
        } else {
            return 0;
        }
    }
});
```





### 29. repartition

repartitionçš„åŠŸèƒ½æ˜¯é‡æ–°åˆ†åŒºï¼Œä¸€å®šä¼šshuffleï¼Œ**å³å°†æ•°æ®éšæœºæ‰“æ•£**ã€‚repartitionçš„åŠŸèƒ½æ˜¯æ”¹å˜åˆ†åŒºæ•°é‡ï¼ˆå¯ä»¥å¢å¤§ã€å‡å°‘ã€ä¸å˜ï¼‰å¯ä»¥å°†æ•°æ®ç›¸å¯¹å‡åŒ€çš„é‡æ–°åˆ†åŒºï¼Œå¯ä»¥æ”¹å–„æ•°æ®å€¾æ–œçš„é—®é¢˜

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
//repartitionæ–¹æ³•ä¸€å®šshuffle
//ä¸è®ºå°†åˆ†åŒºæ•°é‡å˜å¤šã€å˜å°‘ã€æˆ–ä¸å˜ï¼Œéƒ½shuffle
val rdd2 = rdd1.repartition(3)
```



**reparationçš„åº•å±‚è°ƒç”¨çš„æ˜¯coalesceï¼Œshuffle = true**

```scala
coalesce(numPartitions, shuffle = true)
```



### 30. coalesce

coalesceå¯ä»¥shuffleï¼Œä¹Ÿå¯ä»¥ä¸shuffleï¼›

å–å†³äºä¼ å…¥çš„å‚æ•°shuffleæ˜¯å¦ä¸ºtrueï¼›

> å¦‚æœæŠŠåˆ†åŒºæ•°å˜**å¤š**ï¼Œåˆ™shuffleå¿…é¡»ä¸ºtrueæ‰æœ‰æ•ˆæœï¼›å¦åˆ™åˆ†åŒºæ ¹æœ¬æ²¡å˜
>
> å¦‚æœæŠŠåˆ†åŒºæ•°å˜**å°‘ï¼Œ**&#x90A3;ä¹ˆshuffleå¯ä»¥ä¸ºtrueï¼ˆæ•°æ®å°†æ‰“æ•£é‡ç»„ï¼‰ï¼Œä¹Ÿå¯ä»¥falseï¼ˆå¤šä¸ªåˆ†åŒºåˆå¹¶æˆä¸€ä¸ªåˆ†åŒºï¼‰



* shuffle = true

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 3)
//shuffle = true
val rdd2 = rdd1.coalesce(3, true)
//ä¸repartition(3)åŠŸèƒ½ä¸€æ ·
```



* shuffle = false

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
//shuffle = false
val rdd2 = rdd1.coalesce(2, false)
```









&#x20;

# 5 **Actionç®—å­ç¤ºä¾‹**

Actionç®—å­ä¼šè§¦å‘Jobçš„ç”Ÿæˆï¼Œåº•å±‚è°ƒç”¨çš„æ˜¯sparkContext.runJobæ–¹æ³•ï¼Œæ ¹æ®æœ€åä¸€ä¸ªRDDï¼Œä»åå¾€å‰ï¼Œåˆ‡åˆ†Stageï¼Œç”ŸæˆTask

![](images/sparkæ‰§è¡Œè¿‡ç¨‹.png)



### 1. saveAsTextFile

å°†æ•°æ®ä»¥æ–‡æœ¬çš„å½¢å¼ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œä¸€ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªç»“æœæ–‡ä»¶ï¼Œå¯ä»¥æŒ‡å®šhdfsæ–‡ä»¶ç³»ç»Ÿï¼Œä¹Ÿå¯ä»¥æŒ‡å®šæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼ˆæœ¬åœ°æ–‡ä»¶ç³»ç»Ÿè¦å†™file://åè®®ï¼‰ï¼Œæ•°æ®çš„å†™å…¥æ˜¯ä¸‹Executorä¸­Taskå†™å…¥çš„ï¼Œæ˜¯å¤šä¸ªTaskå¹¶è¡Œå†™å…¥çš„ã€‚

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5), 2)
rdd1.saveAsTextFile("hdfs://node-1.51doit.cn:9000/out2")
```



### 2. saveAsNewApiHadoopFile









### 3. collect

æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„Taskï¼Œå°†æ•°æ®åœ¨Executorä¸­ï¼Œå°†æ•°æ®æ•°ç»„çš„å½¢å¼ä¿å­˜åˆ°å†…å­˜ä¸­ï¼Œç„¶åå°†æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„æ•°æ®ä»¥æ•°ç»„å½¢å¼é€šè¿‡ç½‘ç»œæ”¶é›†å›Driverç«¯ï¼Œæ•°æ®æŒ‰ç…§åˆ†åŒºç¼–å·æœ‰åºè¿”å›



```scala

val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
val rdd2 = rdd1.map(_ * 10)
//è°ƒç”¨collectæ–¹æ³•ï¼Œæ˜¯ä¸€ä¸ªAction
val res: Array[Int] = rdd2.collect()
println(res.toBuffer)
```

collectåº•å±‚å®ç°ï¼š

```scala
def collect(): Array[T] = withScope {
  //thisä»£è¡¨æœ€åä¸€ä¸ªRDDï¼Œå³è§¦å‘Actionçš„RDD
  //(iter: Iterator[T]) => iter.toArray å‡½æ•°ä»£è¡¨å¯¹æœ€åä¸€ä¸ªè¿›è¡Œçš„å¤„ç†é€»è¾‘ï¼Œå³å°†æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„è¿­ä»£å™¨ä¸­çš„æ•°æ®è¿­ä»£å¤„å‡ºæ¥ï¼Œæ”¾åˆ°å†…å­˜ä¸­
  //æœ€åå°†æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„æ•°ç»„é€šè¿‡ç½‘ç»œä¼ è¾“åˆ°Driverç«¯
  val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  //åœ¨Driverç«¯ï¼Œå°†å¤šä¸ªæ•°ç»„åˆå¹¶æˆä¸€ä¸ªæ•°ç»„
  Array.concat(results: _*)
}
```

> ä½¿ç”¨collectæ–¹æ³•çš„æ³¨æ„äº‹é¡¹ï¼š
>
> å¦‚æœDriverçš„å†…å­˜ç›¸å¯¹è¾ƒå°ï¼Œå¹¶ä¸”æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„æ•°æ®æ¯”è¾ƒå¤§ï¼Œé€šè¿‡ç½‘ç»œä¼ è¾“çš„æ•°æ®ï¼Œè¿”å›åˆ°Driverï¼Œå½“è¿”å›åˆ°Driverç«¯çš„æ•°æ®è¾¾åˆ°äº†ä¸€å®šå¤§å°ï¼Œå°±ä¸æ”¶é›†äº†ï¼Œå³å°†ä¸€éƒ¨åˆ†æ— æ³•æ”¶é›†çš„æ•°æ®ä¸¢å¼ƒ
>
>
>
> å¦‚æœéœ€è¦å°†å¤§é‡çš„æ•°æ®æ”¶é›†åˆ°Driverç«¯ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨æäº¤ä»»åŠ¡çš„æ—¶å€™æŒ‡å®šDriverçš„å†…å­˜å¤§å° (--driver-memory 2g)







### 4. count

è¿”å›rddå…ƒç´ çš„æ•°é‡ï¼Œå…ˆåœ¨æ¯ä¸ªåˆ†åŒºå†…æ±‚æ•°æ®çš„æ¡æ•°ï¼Œç„¶åå†å°†æ¯ä¸ªåˆ†åŒºè¿”å›çš„æ¡æ•°åœ¨Driverè¿›è¡Œæ±‚å’Œ

```scala
val rdd1 = sc.parallelize(List(5,7 ,9,6,1 ,8,2, 4,3,10), 4)
//åœ¨æ¯ä¸ªåˆ†åŒºå†…å…ˆè®¡ç®—æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„æ•°æ®æ¡æ•°ï¼ˆä½¿ç”¨çš„æ˜¯è¾¹éå†ï¼Œè¾¹è®¡æ•°ï¼‰
//ç„¶åå†å°†æ¯ä¸ªåˆ†åŒºè¿”å›çš„æ¡æ•°ï¼Œåœ¨Driverè¿›è¡Œæ±‚å’Œ
val r: Long = rdd1.count()
```





### 5. aggregate

aggregateæ–¹å¼æ˜¯Actionï¼Œå¯ä»¥å°†å¤šä¸ªåˆ†åŒºçš„æ•°æ®è¿›è¡Œèšåˆè¿ç®—ï¼Œä¾‹å¦‚è¿›è¡Œç›¸åŠ ï¼Œæ¯”è¾ƒå¤§å°ç­‰

> aggregateæ–¹æ³•å¯ä»¥æŒ‡å®šä¸€ä¸ªåˆå§‹å€¼ï¼Œåˆå§‹å€¼åœ¨æ¯ä¸ªåˆ†åŒºè¿›è¡Œèšåˆæ—¶ä¼šåº”ç”¨ä¸€æ¬¡ï¼Œå…¨å±€èšåˆæ—¶ä¼šåœ¨ä½¿ç”¨ä¸€æ¬¡

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)

//f1æ˜¯åœ¨Executorç«¯æ‰§è¡Œçš„
val f1 = (a: Int, b: Int) => {
  println("f1 function invoked ~~~~")
  a + b
}

//f2å®åœ¨Driverç«¯æ‰§è¡Œçš„
val f2 = (m: Int, n: Int) => {
  println("f2 function invoked !!!!")
  m + n
}

//è¿”å›çš„ç»“æœä¸º55
val r1: Int = rdd1.aggregate(0)(f1, f2)

//è¿”å›çš„ç»“æœä¸º50055
val r2: Int = rdd1.aggregate(10000)(f1, f2)

```



```scala
val rdd1 = sc.parallelize(List("a", "b", "c", "d"), 2)
val r: String = rdd1.aggregate("&")(_ + _, _ + _)

//è¿”å›çš„å›çš„æœ‰ä¸¤ç§ï¼šåº”ä¸ºtaskçš„åˆ†å¸ƒå¼å¹¶è¡Œè¿è¡Œçš„ï¼Œå…ˆè¿”å›çš„ç»“æœåœ¨å‰é¢
// &&cd&ab æˆ– &&ab&cd
```



### 6. reduce

å°†æ•°æ®å…ˆåœ¨æ¯ä¸ªåˆ†åŒºå†…è¿›è¡Œå±€éƒ¨èšåˆï¼Œç„¶åå°†æ¯ä¸ªåˆ†åŒºè¿”å›çš„ç»“æœåœ¨Driverç«¯è¿›è¡Œå…¨å±€èšåˆ

```scala
 
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
val f1 = (a: Int, b: Int) => {
  println("f1 function invoked ~~~~")
  a + b
}
//f1è¿™ä¸ªå‡½æ•°å³åœ¨Executorä¸­æ‰§è¡Œï¼Œåˆåœ¨Driverç«¯æ‰§è¡Œ
//reduceæ–¹æ³•å±€éƒ¨èšåˆçš„é€»è¾‘å’Œå…¨å±€èšåˆçš„é€»è¾‘æ˜¯ä¸€æ ·çš„
//å±€éƒ¨èšåˆæ˜¯åœ¨æ¯ä¸ªåˆ†åŒºå†…å®Œæˆï¼ˆExecutorï¼‰
//å…¨å±€èšåˆå®åœ¨Driverå®Œæˆçš„
val r = rdd1.reduce(f1)
```



### 7. sum

&#x20;*sumæ–¹æ³•æ˜¯Actionï¼Œå®ç°çš„é€»è¾‘åªèƒ½æ˜¯ç›¸åŠ *

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
//sumåº•å±‚è°ƒç”¨çš„æ˜¯foldï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªæŸ¯é‡ŒåŒ–æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªæ‹¬å·ä¼ å…¥çš„åˆå§‹å€¼æ˜¯0.0
//ç¬¬äºŒä¸ªæ‹¬å·ä¼ å…¥çš„å‡½æ•°(_ + _) ï¼Œå±€éƒ¨èšåˆå’Œå…¨å±€èšåˆéƒ½æ˜¯ç›¸åŠ 
val r = rdd1.sum()

//sumçš„åº•å±‚å®ç°
val r = rdd1.fold(0.0)(_ + _)
```

**javaç‰ˆæœ¬**

> å¿…é¡»åœ¨æ˜ç¡®çš„JavaDoubleRDD æˆ–  JavaIntRDDä¸Šé¢æ‰èƒ½è°ƒç”¨

```java
JavaDoubleRDD javaDoubleRDD = orderRDD.mapToDouble(od -> od.amt);
Double sum = javaDoubleRDD.sum();
System.out.println(sum);
```





### 8. fold

foldè·Ÿreduceç±»ä¼¼ï¼Œåªä¸è¿‡foldæ˜¯ä¸€ä¸ªæŸ¯é‡ŒåŒ–æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å¯ä»¥æŒ‡å®šä¸€ä¸ªåˆå§‹å€¼

```scala
val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9,10), 4)
//foldä¸reduceæ–¹æ³•ç±»ä¼¼ï¼Œè¯¥æ–¹æ³•æ˜¯ä¸€ä¸ªæŸ¯é‡ŒåŒ–æ–¹æ³•ï¼Œç¬¬ä¸€ä¸ªæ‹¬å·ä¼ å…¥çš„åˆå§‹å€¼æ˜¯0.0
//ç¬¬äºŒä¸ªæ‹¬å·ä¼ å…¥çš„å‡½æ•°(_ + _) ï¼Œå±€éƒ¨èšåˆå’Œå…¨å±€èšåˆéƒ½æ˜¯ç›¸åŠ 
val r = rdd1.fold(0)(_ + _)
```



### 9. minã€max

å°†æ•´ä¸ªRDDä¸­å…¨éƒ¨å¯¹åº”çš„æ•°æ®æ±‚æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼Œåº•å±‚çš„å®ç°æ˜¯ï¼šç°åœ¨æ¯ä¸ªåˆ†åŒºå†…æ±‚æœ€å¤§å€¼æˆ–æœ€å°å€¼ï¼Œç„¶åå°†æ¯ä¸ªåˆ†åŒºè¿”å›çš„æ•°æ®åœ¨Driverç«¯å†è¿›è¡Œæ¯”è¾ƒï¼ˆminã€maxæ²¡æœ‰shuffleï¼‰

```scala
val rdd1 = sc.parallelize(List(5,7 ,9,6,1 ,8,2, 4,3,10), 4)
//æ²¡æœ‰shuffle
val r: Int = rdd1.max()
```



### 10. take

è¿”å›ä¸€ä¸ªç”±æ•°æ®é›†çš„å‰nä¸ªå…ƒç´ ç»„æˆçš„æ•°ç»„ï¼Œå³ä»RDDçš„0å·åˆ†åŒºå¼€å§‹å–æ•°æ®ï¼Œtakeå¯èƒ½è§¦å‘ä¸€åˆ°å¤šæ¬¡Actionï¼ˆå¯èƒ½ç”Ÿæˆå¤šä¸ªJobï¼‰å› ä¸ºé¦–å…ˆä»0å·åˆ†åŒºå–æ•°æ®ï¼Œå¦‚æœå–å¤Ÿäº†ï¼Œå°±ç›´æ¥è¿”å›ï¼Œæ²¡æœ‰å–å¤Ÿï¼Œå†è§¦å‘Actionï¼Œä»åé¢çš„åˆ†åŒºç»§ç»­å–æ•°æ®ï¼Œç›´åˆ°å–å¤ŸæŒ‡å®šçš„æ¡æ•°ä¸ºæ­¢

```scala
val rdd1 = sc.parallelize(List(5,7 ,9,6,1 ,8,2, 4,3,10), 4)
//å¯èƒ½ä¼šè§¦å‘ä¸€åˆ°å¤šæ¬¡Action
val res: Array[Int] = rdd1.take(2)
```

&#x20;

### 11. first

è¿”å›RDDä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œç±»ä¼¼äºtake(1)ï¼Œfirstè¿”å›çš„ä¸æ˜¯æ•°ç»„

```scala
val rdd1 = sc.parallelize(List(5,7 ,9,6,1 ,8,2, 4,3,10), 4)
//è¿”å›RDDä¸­å¯¹åº”çš„ç¬¬ä¸€æ¡æ•°æ®
val r: Int = rdd1.first()
```



### 12. top

å°†RDDä¸­æ•°æ®æŒ‰ç…§é™åºæˆ–è€…æŒ‡å®šçš„æ’åºè§„åˆ™ï¼Œè¿”å›å‰nä¸ªå…ƒç´ 



```scala
val rdd1 = sc.parallelize(List(
  5, 7, 6, 4,
  9, 6, 1, 7,
  8, 2, 8, 5,
  4, 3, 10, 9
), 4)

val res1: Array[Int] = rdd1.top(2)
//æŒ‡å®šæ’åºè§„åˆ™ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤çš„æ’åºè§„åˆ™
implicit val ord = Ordering[Int].reverse
val res2: Array[Int] = rdd1.top(2)
val res3: Array[Int] = rdd1.top(2)(Ordering[Int].reverse)
```

**topåº•å±‚è°ƒç”¨çš„ä½¿ç”¨takeOrdered**

```scala
def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
  takeOrdered(num)(ord.reverse)
}
```



### 13. takeOrdered

topåº•å±‚è°ƒçš„æ˜¯takeOrderedï¼ŒtakeOrderedæ›´çµæ´»ï¼Œå¯ä»¥ä¼ æŒ‡å®šæ’åºè§„åˆ™ã€‚åº•å±‚æ˜¯å…ˆåœ¨æ¯ä¸ªåˆ†åŒºå†…æ±‚topNï¼Œç„¶åå°†æ¯ä¸ªåˆ†åŒºè¿”å›çš„ç»“æœå†åœ¨Diverç«¯æ±‚topN

> åœ¨æ¯ä¸ªåˆ†åŒºå†…è¿›è¡Œæ’åºï¼Œä½¿ç”¨çš„æ˜¯æœ‰ç•Œä¼˜å…ˆé˜Ÿåˆ—ï¼Œç‰¹ç‚¹æ˜¯æ•°æ®æ·»åŠ åˆ°å…¶ä¸­ï¼Œå°±ä¼šæŒ‰ç…§æŒ‡å®šçš„æ’åºè§„åˆ™æ’åºï¼Œå¹¶ä¸”å…è®¸æ•°æ®é‡å¤ï¼Œæœ€å¤šåªå­˜æ”¾æœ€å¤§æˆ–æœ€å°çš„Nä¸ªå…ƒç´ 



```scala
def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
  if (num == 0) {
    Array.empty
  } else {
    val mapRDDs = mapPartitions { items =>
      // Priority keeps the largest elements, so let's reverse the ordering.
      //ä½¿ç”¨æœ‰ç•Œä¼˜å…ˆé˜Ÿåˆ—
      val queue = new BoundedPriorityQueue[T](num)(ord.reverse)
      queue ++= collectionUtils.takeOrdered(items, num)(ord)
      Iterator.single(queue)
    }
    if (mapRDDs.partitions.length == 0) {
      Array.empty
    } else {
      mapRDDs.reduce { (queue1, queue2) =>
        queue1 ++= queue2 //å°†å¤šä¸ªæœ‰ç•Œä¼˜å…ˆé˜Ÿåˆ—è¿›è¡Œ++= ï¼Œè¿”å›ä¸¤ä¸ªæœ‰ç•Œä¼˜å…ˆé˜Ÿåˆ—æœ€å¤§çš„Nä¸ª
        queue1
      }.toArray.sorted(ord)
    }
  }
}
```

&#x20;&#x20;

### 14. foreach&#x20;

å°†æ•°æ®ä¸€æ¡ä¸€æ¡çš„å–å‡ºæ¥è¿›è¡Œå¤„ç†ï¼Œå‡½æ•°æ²¡æœ‰è¿”å›

```scala
val sc = SparkUtil.getContext("FlowCount", true)

val rdd1 = sc.parallelize(List(
  5, 7, 6, 4,
  9, 6, 1, 7,
  8, 2, 8, 5,
  4, 3, 10, 9
), 4)

rdd1.foreach(e => {
  println(e * 10) //å‡½æ•°æ˜¯åœ¨Executorä¸­æ‰§è¡Œ
})
```

ä½¿ç”¨foreachå°†æ•°æ®å†™å…¥åˆ°MySQLä¸­ï¼Œä¸å¥½ ï¼Œæ•ˆç‡ä½

```scala
rdd1.foreach(e => {
  //ä½†æ˜¯ä¸å¥½ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
  //æ¯å†™ä¸€æ¡æ•°æ®ç”¨ä¸€ä¸ªè¿æ¥å¯¹è±¡ï¼Œæ•ˆç‡å¤ªä½äº†
  val connection = DriverManager.getConnection("jdbc:mysql://node-1.51doit.cn:3306/doit35?characterEncoding=utf-8", "root", "123456")
  val preparedStatement = connection.prepareStatement("Insert into tb_res values (?)")
  preparedStatement.setInt(1, e)
  preparedStatement.executeUpdate()
})
```



### 15. foreachPartition

å’Œforeachç±»ä¼¼ï¼Œåªä¸è¿‡æ˜¯ä»¥åˆ†åŒºä½å•ä½ï¼Œä¸€ä¸ªåˆ†åŒºå¯¹åº”ä¸€ä¸ªè¿­ä»£å™¨ï¼Œåº”ç”¨å¤–éƒ¨ä¼ çš„å‡½æ•°ï¼Œå‡½æ•°æ²¡æœ‰è¿”å›å€¼ï¼Œé€šå¸¸ä½¿ç”¨è¯¥æ–¹æ³•å°†æ•°æ®å†™å…¥åˆ°å¤–éƒ¨å­˜å‚¨ç³»ç»Ÿä¸­ï¼Œä¸€ä¸ªåˆ†åŒºè·å–ä¸€ä¸ªè¿æ¥ï¼Œæ•ˆç‡æ›´é«˜

```scala
rdd1.foreachPartition(it => {
  //å…ˆåˆ›å»ºå¥½ä¸€ä¸ªè¿æ¥å¯¹è±¡
  val connection = DriverManager.getConnection("jdbc:mysql://node-1.51doit.cn:3306/doit35?characterEncoding=utf-8", "root", "123456")
  val preparedStatement = connection.prepareStatement("Insert into tb_res values (?)")
  //ä¸€ä¸ªåˆ†åŒºä¸­çš„å¤šæ¡æ•°æ®ç”¨ä¸€ä¸ªè¿æ¥è¿›è¡Œå¤„ç†
  it.foreach(e => {
    preparedStatement.setInt(1, e)
    preparedStatement.executeUpdate()
  })
  //ç”¨å®Œåå…³é—­è¿æ¥
  preparedStatement.close()
  connection.close()
})
```

**javaç‰ˆæœ¬ç¤ºä¾‹&#x20;**

```java

public class _18_ForeachPartition {
    public static void main(String[] args) {

        SparkConf conf = new SparkConf();
        conf.setAppName("xxx");
        conf.setMaster("local");

        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD<String> strRdd = sc.textFile("spark_data/excersize_3/input/order.data");

        // æ”¶é›†: 1ï¼Œä¸»è¦ç”¨äºæµ‹è¯•ï¼›
        // 2ï¼Œæœ‰æ—¶å€™å¤æ‚çš„ä½œä¸šä¸­ï¼Œéœ€è¦å…ˆç”¨rddå»åšä¸€ä¸ªè¿ç®—ï¼Œå¾—åˆ°å°‘é‡ç»“æœåæ”¶é›†åˆ°driverç«¯
        // ç„¶åè¿™ä¸ªæ”¶é›†åˆ°çš„æ•°æ®é›†åˆï¼Œå¯ä»¥å¹¿æ’­ç»™ä¸‹ä¸€ä¸ªrddè¿ç®—ä½œä¸šä½¿ç”¨
        // è¦æ³¨æ„çš„ç‚¹ï¼š
        List<String> collect = strRdd.collect();
        System.out.println(collect);

        long count = strRdd.count();
        System.out.println(count);

        // {"uid":1,"oid":"o_1","pid":1,"amt":78.8}
        // {"uid":1,"oid":"o_2","pid":2,"amt":68.8}
        JavaRDD<Order> orderRDD = strRdd.map(s -> JSON.parseObject(s, Order.class));

        JavaPairRDD<Integer, Order> kvRdd = orderRDD.keyBy(od -> od.pid);


        // ç»Ÿè®¡: æ¯ç§å•†å“çš„ è´­ä¹°å•æ•°ã€æ€»é¢ã€äººæ•°,å†™å…¥ mysql
        JavaPairRDD<Integer, Agg> aggRdd = kvRdd.aggregateByKey(
                new Agg(),
                (agg, od) -> {
                    agg.order_count++;
                    agg.amt = agg.amt.add(BigDecimal.valueOf(od.amt));
                    agg.userBitmap.add(od.uid);


                    return agg;
                },
                (agg1, agg2) -> {
                    agg1.order_count += agg2.order_count;
                    agg1.userBitmap.or(agg2.userBitmap);
                    agg1.amt = agg1.amt.add(agg2.amt);
                    return agg1;
                }
        );

        // åˆ©ç”¨ foreachPartitionæ¥è¾“å‡º
        aggRdd.foreachPartition(new VoidFunction<Iterator<Tuple2<Integer, Agg>>>() {
            @Override
            public void call(Iterator<Tuple2<Integer, Agg>> partitionIterator) throws Exception {

                Connection connection = DriverManager.getConnection("jdbc:mysql://doitedu01:3306/doit50", "root", "ABC123.abc123");
                PreparedStatement stmt = connection.prepareStatement("insert into order_tj values (?,?,?,?)");


                while (partitionIterator.hasNext()) {
                    Tuple2<Integer, Agg> kv = partitionIterator.next();
                    Integer pid = kv._1;
                    Agg agg = kv._2;

                    // å°†aggä¸­çš„ä¿¡æ¯ï¼Œå†™å…¥æ•°æ®è¡¨
                    stmt.setInt(1, pid);
                    stmt.setInt(2, agg.order_count);
                    stmt.setInt(3, agg.userBitmap.getCardinality());
                    stmt.setBigDecimal(4, agg.amt);

                    stmt.execute();

                }

                stmt.close();
                connection.close();
            }
        });


    }


    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    public static class Order implements Serializable {
        private int uid;
        private String oid;
        private int pid;
        private double amt;
    }

    @Data
    @NoArgsConstructor
    @AllArgsConstructor
    public static class Agg implements Serializable {
        private int order_count;
        private BigDecimal amt = BigDecimal.ZERO;
        RoaringBitmap userBitmap = RoaringBitmap.bitmapOf();
    }


}
```





# 6 **å…¶ä»–ç®—å­**

### 16. cacheã€persist

å°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜ï¼ˆExecutorçš„å†…å­˜ï¼‰ï¼Œç¬¬ä¸€æ¬¡è§¦å‘Actionï¼Œæ‰ä¼šå°†æ•°æ®è¿›è¡Œè¿ç®—ç„¶åæ”¾å…¥åˆ°å†…å­˜ï¼Œä»¥ååœ¨è§¦å‘Actionï¼Œå¯ä»¥å¤ç”¨å‰é¢å†…å­˜ä¸­ç¼“å­˜çš„æ•°æ®ï¼Œå¯ä»¥æå‡æ‰§è¡Œçš„æ•ˆç‡

cacheå’Œpersistçš„ä½¿ç”¨åœºæ™¯ï¼šä¸€ä¸ªapplicationå¤šæ¬¡è§¦å‘Actionï¼Œä¸ºäº†å¤ç”¨å‰é¢RDDè®¡ç®—å¥½çš„æ•°æ®ï¼Œé¿å…åå¤è¯»å–HDFSï¼ˆæ•°æ®æºï¼‰ä¸­çš„æ•°æ®å’Œé‡å¤è®¡ç®—ï¼Œå¯ä»¥å°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜æˆ–ç£ç›˜ã€executoræ‰€åœ¨çš„ç£ç›˜ã€‘ï¼Œç¬¬ä¸€æ¬¡è§¦å‘actionæ‰æ”¾å…¥åˆ°å†…å­˜æˆ–ç£ç›˜ï¼Œä»¥åä¼šç¼“å­˜çš„RDDè¿›è¡Œæ“ä½œå¯ä»¥å¤ç”¨ç¼“å­˜çš„æ•°æ®ã€‚

ä¸€ä¸ªRDDå¤šæ¬¡è§¦å‘Actionç¼“å­˜æ‰æœ‰æ„ä¹‰ï¼Œå¦‚æœå°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜ï¼Œå†…å­˜ä¸å¤Ÿï¼Œä»¥åˆ†åŒºä½å•ä½ï¼Œåªç¼“å­˜éƒ¨åˆ†åˆ†åŒºçš„æ•°æ®ï¼Œcacheåº•å±‚è°ƒç”¨persistï¼Œå¯ä»¥æŒ‡å®šæ›´åŠ ä¸°å¯Œçš„å­˜å‚¨çº§åˆ«ï¼Œæ”¯æŒå¤šç§StageLevelï¼Œå¯ä»¥å°†æ•°æ®åºåˆ—åŒ–,é»˜è®¤æ”¾å…¥å†…å­˜ä½¿ç”¨çš„æ˜¯javaå¯¹è±¡å­˜å‚¨ï¼Œä½†æ˜¯å ç”¨ç©ºé—´å¤§ï¼Œä¼˜ç‚¹é€Ÿåº¦å¿«ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–çš„åºåˆ—åŒ–æ–¹å¼

cacheå’Œpersistæ–¹æ³•ï¼Œä¸¥æ ¼æ¥è¯´ï¼Œä¸æ˜¯Transformationï¼Œåº”ä¸ºæ²¡æœ‰ç”Ÿæˆæ–°çš„RDDï¼Œåªæ˜¯æ ‡è®°å½“å‰rddè¦cacheæˆ–persist



è®¾ç½®æŒ‡å®šçš„åºåˆ—åŒ–æ–¹å¼ï¼Œç„¶åæ”¾åˆ°å†…å­˜ä¸­ï¼ˆsparké»˜è®¤ä½¿ç”¨javaå¯¹è±¡çš„å½¢å¼ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸ºkyroåºåˆ—åŒ–æ–¹å¼ï¼‰

```scala
 conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
```



cache() æœ¬è´¨ä¸Šå°±æ˜¯ç­‰ä»·äºï¼š

```java
persist(StorageLevel.MEMORY_ONLY)
```

StorageLevel ï¼š æŒ‡çš„æ˜¯ï¼Œç¼“å­˜çš„å­˜å‚¨çº§åˆ«

æ‰€æœ‰levelå¦‚ä¸‹ï¼š

```java
// å‚æ•°ï¼š æ˜¯å¦æ”¾ç£ç›˜ï¼Œæ˜¯å¦æ”¾å†…å­˜ï¼Œæ˜¯å¦æ”¾å †å¤–ï¼Œæ˜¯å¦ååºåˆ—åŒ–ï¼Œå‰¯æœ¬æ•°
val NONE = new StorageLevel(false, false, false, false)
val DISK_ONLY = new StorageLevel(true, false, false, false)
val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
val DISK_ONLY_3 = new StorageLevel(true, false, false, false, 3)
val MEMORY_ONLY = new StorageLevel(false, true, false, true)
val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
val OFF_HEAP = new StorageLevel(true, true, true, false, 1)
```



&#x20;





### 17. checkpoint

checkpointä½¿ç”¨åœºæ™¯ï¼šé€‚åˆå¤æ‚çš„è®¡ç®—ã€æœºå™¨å­¦ä¹ ã€‘ï¼Œä¸ºäº†é¿å…ä¸­é—´ç»“æœæ•°æ®ä¸¢å¤±é‡å¤è®¡ç®—ï¼Œå¯ä»¥å°†å®è´µçš„ä¸­é—´ç»“æœä¿å­˜åˆ°hdfsä¸­ï¼Œä¿è¯ä¸­é—´ç»“æœå®‰å…¨ã€‚

åœ¨è°ƒç”¨rddçš„checkpintæ–¹æ³•ä¹‹å‰ï¼Œä¸€å®šè¦æŒ‡å®šcheckpointçš„ç›®å½•sc.setCheckPointDirï¼ŒæŒ‡çš„HDFSå­˜å‚¨ç›®å½•ï¼Œä¸ºä¿è¯ä¸­é—´ç»“æœå®‰å…¨ï¼Œå°†æ•°æ®ä¿å­˜åˆ°HDFSä¸­

ç¬¬ä¸€æ¬¡è§¦å‘Actionï¼Œæ‰åšcheckpointï¼Œä¼šé¢å¤–è§¦å‘ä¸€ä¸ªjobï¼Œè¿™ä¸ªjobçš„ç›®çš„å°±æ˜¯å°†ç»“æœä¿å­˜åˆ°HDFSä¸­

å¦‚æœRDDåšäº†checkpointï¼Œè¿™ä¸ªRDDä»¥å‰çš„ä¾èµ–å…³ç³»å°±ä¸å†ä½¿ç”¨äº†ï¼Œè§¦å‘å¤šæ¬¡Actionï¼Œcheckpointæ‰æœ‰æ„ä¹‰ï¼Œå¤šç”¨äºè¿­ä»£è®¡ç®—

checkpointä¸¥æ ¼çš„è¯´ï¼Œä¸æ˜¯Transformationï¼Œåªæ˜¯æ ‡è®°å½“å‰RDDè¦åšcheckpoint



# 7 ç¼–ç¨‹ç»ƒä¹ 

## 1. ç”¨æˆ·æ´»è·ƒåˆ†æ

æœ‰å¦‚ä¸‹æ•°æ®å­˜äºæ–‡ä»¶ä¸­ï¼Œæ–‡ä»¶ä¸­çš„å„å­—æ®µåˆ†åˆ«ä¸ºï¼š

> id, name, login\_date, device\_type, gender, stay\_long

```java
1,aaa,2024-08-01,android,male,5
1,aaa,2024-08-01,android,male,5
1,aaa,2024-08-01,android,male,5
2,bbb,2024-08-01,android,male,3
2,bbb,2024-08-01,android,male,3
3,ccc,2024-08-01,ios,male,2
4,ddd,2024-08-01,ios,female,6
1,aaa,2024-08-02,android,male,4
2,bbb,2024-08-02,android,male,2
3,ccc,2024-08-02,ios,male,2
5,eee,2024-08-02,ios,female,5
1,aaa,2024-08-03,android,male,5
1,aaa,2024-08-03,android,male,5
5,eee,2024-08-03,ios,female,5
1,aaa,2024-08-04,ios,male,6
3,ccc,2024-08-04,ios,male,6
2,bbb,2024-08-05,ios,male,6
1,aaa,2024-08-05,ios,male,2
1,aaa,2024-08-05,ios,male,2
2,bbb,2024-08-06,android,male,1
```

ä½¿ç”¨sparkç¼–ç¨‹ï¼Œå®ç°å¦‚ä¸‹ç»Ÿè®¡

* 2024-08æœˆ çš„æ€»ç™»å½•ç”¨æˆ·æ•°

* å„æ€§åˆ«ç”¨æˆ·çš„æ€»è®¿é—®æ—¶é•¿ &#x20;

* å„æ€§åˆ«ç”¨æˆ·çš„å¹³å‡è®¿é—®æ—¶é•¿

* å‘ç”Ÿè¿‡è¿ç»­3å¤©åŠä»¥ä¸Šç™»å½•çš„ç”¨æˆ·



## 2. æ¼æ–—æ¨¡å‹è®¡ç®—

ä»»ä¸€ä¸ªä¸šåŠ¡ç›®æ ‡ï¼ˆå¦‚ä¸‹å•ã€æˆ–æ”¯ä»˜ã€æˆ–æ³¨å†Œã€æˆ–å‚ä¸æŠ½å¥–ç­‰ï¼‰ï¼Œå¾€å¾€æœ‰ä¸€æ¡æˆ–å¤šæ¡åˆ°è¾¾çš„è·¯å¾„ï¼›å¦‚ï¼š

* é¦–é¡µæœç´¢  --> æµè§ˆå•†å“è¯¦æƒ… --> æ·»åŠ è´­ç‰©è½¦ -->  ä¸‹å•

* é¦–é¡µè½®æ’­å›¾ç‚¹å‡» --> é¢†åˆ¸ --> æ·»åŠ è´­ç‰©è½¦ --> ä¸‹å•

å¦‚ä¸‹æ•°æ®ä»£è¡¨ç”¨æˆ·åœ¨appä¸Šçš„å„ç§è¡Œä¸ºè®°å½•

```json
{"uid":1,"event_time":1725886211000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886224000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886232000,"event_id":"e05","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886244000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886248000,"event_id":"e06","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886252000,"event_id":"e07","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":1,"event_time":1725886263000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"andorid"}
{"uid":2,"event_time":1725886312000,"event_id":"e02","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886319000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886321000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886326000,"event_id":"e05","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886362000,"event_id":"e07","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886369000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":2,"event_time":1725886412000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":3,"event_time":1725886312000,"event_id":"e01","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886319000,"event_id":"e02","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886321000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886326000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886362000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886369000,"event_id":"e05","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":3,"event_time":1725886412000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"android"}
{"uid":4,"event_time":1725886312000,"event_id":"e02","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886319000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886321000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886326000,"event_id":"e05","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886362000,"event_id":"e07","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886369000,"event_id":"e03","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
{"uid":4,"event_time":1725886412000,"event_id":"e04","properties":{"url":"/aaa/bbb"},"device_type":"ios"}
```

ç°å®šä¹‰äº†ä¸€æ¡ä¸šåŠ¡è·¯å¾„ï¼ˆæ¼æ–—æ¨¡å‹ï¼‰ä¸ºï¼š

e03 -> e05 -> e06  ï¼ˆå³è¿™æ¡ä¸šåŠ¡è·¯å¾„ä¸­çš„è§¦è¾¾æ­¥éª¤ä¸ºï¼Œå…ˆå‘ç”Ÿe3è¡Œä¸ºï¼Œåé¢å‘ç”Ÿe5è¡Œä¸ºï¼Œå†åé¢å‘ç”Ÿe6è¡Œä¸ºï¼‰

ä½¿ç”¨sparkå®ç°å¦‚ä¸‹ç»Ÿè®¡ï¼š

* å„è®¾å¤‡ç±»å‹ä¸‹ï¼Œæ¼æ–—æ¨¡å‹ä¸­**å„æ­¥éª¤**çš„ï¼šè§¦è¾¾äººæ•°ï¼Œç›¸å¯¹è½¬åŒ–ç‡ï¼Œç»å¯¹è½¬åŒ–ç‡







