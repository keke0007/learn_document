# å¤§æ•°æ®ç¦»çº¿ä¸å®æ—¶å¼€å‘å­¦ä¹ æŒ‡å—

## ğŸ“š é¡¹ç›®æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†å®Œæ•´çš„å¤§æ•°æ®ç¦»çº¿ä¸å®æ—¶å¼€å‘å­¦ä¹ èµ„æºï¼ŒåŒ…æ‹¬ Hadoopã€Sparkã€Flinkã€Kafka ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œæ¶µç›–ç¦»çº¿æ‰¹å¤„ç†å’Œå®æ—¶æµå¤„ç†ï¼Œå¸®åŠ©ä½ ç³»ç»ŸæŒæ¡å¤§æ•°æ®å¼€å‘æŠ€æœ¯ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
bigdata/
â”œâ”€â”€ GUIDE.md                     # æœ¬æŒ‡å—æ–‡æ¡£ï¼ˆå¿«é€Ÿå…¥é—¨ï¼‰
â”œâ”€â”€ README.md                    # å¤§æ•°æ®å¼€å‘çŸ¥è¯†ç‚¹æ€»è§ˆï¼ˆè¯¦ç»†æ–‡æ¡£ï¼‰
â”œâ”€â”€ cases/                       # å®æˆ˜æ¡ˆä¾‹ç›®å½•
â”‚   â”œâ”€â”€ hadoop_ecosystem.md     # æ¡ˆä¾‹1ï¼šHadoop ç”Ÿæ€ç³»ç»Ÿ
â”‚   â”œâ”€â”€ spark_batch.md          # æ¡ˆä¾‹2ï¼šSpark ç¦»çº¿æ‰¹å¤„ç†
â”‚   â”œâ”€â”€ flink_streaming.md      # æ¡ˆä¾‹3ï¼šFlink å®æ—¶æµå¤„ç†
â”‚   â”œâ”€â”€ kafka_streaming.md      # æ¡ˆä¾‹4ï¼šKafka æµå¼æ•°æ®
â”‚   â”œâ”€â”€ hive_hbase.md           # æ¡ˆä¾‹5ï¼šHive å’Œ HBase
â”‚   â””â”€â”€ data_pipeline.md        # æ¡ˆä¾‹6ï¼šæ•°æ®ç®¡é“
â”œâ”€â”€ data/                        # éªŒè¯æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ sample_data.csv          # ç¤ºä¾‹æ•°æ®
â”‚   â”œâ”€â”€ streaming_data.json      # æµå¼æ•°æ®
â”‚   â””â”€â”€ performance_test.txt     # æ€§èƒ½æµ‹è¯•æ•°æ®
â””â”€â”€ scripts/                     # ä»£ç ç¤ºä¾‹ç›®å½•
    â”œâ”€â”€ spark_batch.py           # Spark æ‰¹å¤„ç†ç¤ºä¾‹
    â”œâ”€â”€ flink_streaming.java     # Flink æµå¤„ç†ç¤ºä¾‹
    â”œâ”€â”€ kafka_producer.py        # Kafka ç”Ÿäº§è€…ç¤ºä¾‹
    â””â”€â”€ hive_query.hql           # Hive æŸ¥è¯¢ç¤ºä¾‹
```

---

## ğŸ¯ å­¦ä¹ è·¯å¾„

### é˜¶æ®µä¸€ï¼šHadoop ç”Ÿæ€ç³»ç»Ÿï¼ˆ10-14å¤©ï¼‰
1. **HDFS**
   - åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
   - æ•°æ®å­˜å‚¨å’Œè¯»å–
   - å‰¯æœ¬æœºåˆ¶

2. **MapReduce**
   - MapReduce ç¼–ç¨‹æ¨¡å‹
   - ä½œä¸šè°ƒåº¦
   - æ€§èƒ½ä¼˜åŒ–

3. **YARN**
   - èµ„æºç®¡ç†
   - ä»»åŠ¡è°ƒåº¦
   - é›†ç¾¤ç®¡ç†

### é˜¶æ®µäºŒï¼šSpark ç¦»çº¿æ‰¹å¤„ç†ï¼ˆ10-14å¤©ï¼‰
1. **Spark Core**
   - RDD ç¼–ç¨‹
   - è½¬æ¢å’Œè¡ŒåŠ¨æ“ä½œ
   - æŒä¹…åŒ–æœºåˆ¶

2. **Spark SQL**
   - DataFrame å’Œ Dataset
   - SQL æŸ¥è¯¢
   - æ•°æ®æºé›†æˆ

3. **Spark ä¼˜åŒ–**
   - åˆ†åŒºä¼˜åŒ–
   - å¹¿æ’­å˜é‡
   - æ•°æ®å€¾æ–œå¤„ç†

### é˜¶æ®µä¸‰ï¼šFlink å®æ—¶æµå¤„ç†ï¼ˆ10-14å¤©ï¼‰
1. **Flink åŸºç¡€**
   - DataStream API
   - çª—å£æ“ä½œ
   - æ—¶é—´è¯­ä¹‰

2. **æµå¤„ç†**
   - äº‹ä»¶æ—¶é—´å¤„ç†
   - çŠ¶æ€ç®¡ç†
   - å®¹é”™æœºåˆ¶

3. **Flink SQL**
   - æµå¼ SQL
   - è¡¨è¿æ¥
   - åŠ¨æ€è¡¨

### é˜¶æ®µå››ï¼šKafka æµå¼æ•°æ®ï¼ˆ7-10å¤©ï¼‰
1. **Kafka åŸºç¡€**
   - Topic å’Œ Partition
   - Producer å’Œ Consumer
   - æ¶ˆæ¯å­˜å‚¨

2. **Kafka Streams**
   - æµå¤„ç†åº”ç”¨
   - çŠ¶æ€å­˜å‚¨
   - çª—å£æ“ä½œ

### é˜¶æ®µäº”ï¼šæ•°æ®å­˜å‚¨ï¼ˆ7-10å¤©ï¼‰
1. **Hive**
   - æ•°æ®ä»“åº“
   - HQL æŸ¥è¯¢
   - åˆ†åŒºå’Œåˆ†æ¡¶

2. **HBase**
   - NoSQL æ•°æ®åº“
   - åˆ—æ—è®¾è®¡
   - è¯»å†™ä¼˜åŒ–

### é˜¶æ®µå…­ï¼šæ•°æ®ç®¡é“ï¼ˆ7-10å¤©ï¼‰
1. **æ•°æ®é‡‡é›†**
   - Flume
   - Sqoop
   - DataX

2. **æ•°æ®ç®¡é“**
   - ETL æµç¨‹
   - æ•°æ®è´¨é‡
   - ç›‘æ§å‘Šè­¦

---

## ğŸ“– æ ¸å¿ƒçŸ¥è¯†ç‚¹è¯¦è§£

### 1. Hadoop ç”Ÿæ€ç³»ç»Ÿ

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Hadoop æ˜¯å¤§æ•°æ®çš„åŸºç¡€æ¡†æ¶ï¼ŒåŒ…æ‹¬ HDFSã€MapReduceã€YARN ç­‰æ ¸å¿ƒç»„ä»¶ã€‚

#### HDFS

**æ ¸å¿ƒæ¦‚å¿µ**
- NameNodeï¼šå…ƒæ•°æ®ç®¡ç†
- DataNodeï¼šæ•°æ®å­˜å‚¨
- å‰¯æœ¬æœºåˆ¶ï¼šé»˜è®¤3å‰¯æœ¬
- å—å¤§å°ï¼š128MBï¼ˆHadoop 2.xï¼‰

**å¸¸ç”¨å‘½ä»¤**
```bash
# æ–‡ä»¶æ“ä½œ
hdfs dfs -ls /
hdfs dfs -put local.txt /data/
hdfs dfs -get /data/remote.txt
hdfs dfs -mkdir -p /data/input
hdfs dfs -rm /data/old.txt

# æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿ
hdfs dfs -df -h
hdfs dfsadmin -report
```

#### MapReduce

**ç¼–ç¨‹æ¨¡å‹**
- Map é˜¶æ®µï¼šæ•°æ®æ˜ å°„
- Shuffle é˜¶æ®µï¼šæ•°æ®æ’åºå’Œåˆ†ç»„
- Reduce é˜¶æ®µï¼šæ•°æ®èšåˆ

**æ¡ˆä¾‹ä»£ç **

```java
// WordCount.java
public class WordCount {
    public static class TokenizerMapper 
        extends Mapper<Object, Text, Text, IntWritable> {
        
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        
        public void map(Object key, Text value, Context context) 
            throws IOException, InterruptedException {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens()) {
                word.set(itr.nextToken());
                context.write(word, one);
            }
        }
    }
    
    public static class IntSumReducer 
        extends Reducer<Text, IntWritable, Text, IntWritable> {
        
        private IntWritable result = new IntWritable();
        
        public void reduce(Text key, Iterable<IntWritable> values, 
                          Context context) 
            throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }
}
```

---

### 2. Spark ç¦»çº¿æ‰¹å¤„ç†

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Spark æ˜¯åŸºäºå†…å­˜è®¡ç®—çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œé€‚åˆç¦»çº¿æ‰¹å¤„ç†å’Œäº¤äº’å¼æŸ¥è¯¢ã€‚

#### Spark Core

**RDD æ“ä½œ**
```python
# spark_batch.py
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("WordCount")
sc = SparkContext(conf=conf)

# è¯»å–æ•°æ®
lines = sc.textFile("hdfs://namenode:9000/data/input.txt")

# è½¬æ¢æ“ä½œ
words = lines.flatMap(lambda line: line.split(" "))
wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)

# è¡ŒåŠ¨æ“ä½œ
result = wordCounts.collect()
for word, count in result:
    print(f"{word}: {count}")

sc.stop()
```

#### Spark SQL

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkSQL") \
    .getOrCreate()

# è¯»å–æ•°æ®
df = spark.read.csv("hdfs://namenode:9000/data/users.csv", 
                   header=True, inferSchema=True)

# SQL æŸ¥è¯¢
df.createOrReplaceTempView("users")
result = spark.sql("""
    SELECT department, COUNT(*) as count, AVG(salary) as avg_salary
    FROM users
    GROUP BY department
    ORDER BY count DESC
""")

result.show()
```

---

### 3. Flink å®æ—¶æµå¤„ç†

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Flink æ˜¯æµå¼å¤„ç†æ¡†æ¶ï¼Œæ”¯æŒä½å»¶è¿Ÿã€é«˜ååçš„å®æ—¶æ•°æ®å¤„ç†ã€‚

#### Flink DataStream

```java
// flink_streaming.java
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.time.Time;

public class FlinkStreaming {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // è¯»å– Kafka æ•°æ®æº
        DataStream<String> stream = env
            .addSource(new FlinkKafkaConsumer<>("topic", 
                new SimpleStringSchema(), kafkaProps));
        
        // æ•°æ®å¤„ç†
        stream
            .map(new MapFunction<String, Tuple2<String, Integer>>() {
                @Override
                public Tuple2<String, Integer> map(String value) {
                    String[] parts = value.split(",");
                    return new Tuple2<>(parts[0], Integer.parseInt(parts[1]));
                }
            })
            .keyBy(0)
            .timeWindow(Time.minutes(5))
            .sum(1)
            .print();
        
        env.execute("Flink Streaming Job");
    }
}
```

---

### 4. Kafka æµå¼æ•°æ®

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Kafka æ˜¯åˆ†å¸ƒå¼æ¶ˆæ¯é˜Ÿåˆ—ï¼Œç”¨äºæ„å»ºå®æ—¶æ•°æ®ç®¡é“å’Œæµå¼åº”ç”¨ã€‚

#### Kafka Producer

```python
# kafka_producer.py
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# å‘é€æ¶ˆæ¯
for i in range(100):
    message = {
        'id': i,
        'timestamp': '2024-01-26 10:00:00',
        'value': i * 10
    }
    producer.send('test-topic', message)

producer.flush()
producer.close()
```

#### Kafka Consumer

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'test-topic',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    auto_offset_reset='earliest',
    enable_auto_commit=True
)

for message in consumer:
    print(f"Received: {message.value}")
```

---

### 5. Hive æ•°æ®ä»“åº“

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Hive æ˜¯åŸºäº Hadoop çš„æ•°æ®ä»“åº“å·¥å…·ï¼Œæä¾› SQL æŸ¥è¯¢èƒ½åŠ›ã€‚

#### Hive æŸ¥è¯¢

```sql
-- hive_query.hql
-- åˆ›å»ºè¡¨
CREATE TABLE IF NOT EXISTS users (
    id BIGINT,
    name STRING,
    age INT,
    department STRING,
    salary DECIMAL(10,2)
)
PARTITIONED BY (dt STRING)
STORED AS PARQUET;

-- åŠ è½½æ•°æ®
LOAD DATA INPATH 'hdfs://namenode:9000/data/users/' 
INTO TABLE users PARTITION (dt='2024-01-26');

-- æŸ¥è¯¢
SELECT department, 
       COUNT(*) as count,
       AVG(salary) as avg_salary,
       MAX(salary) as max_salary
FROM users
WHERE dt = '2024-01-26'
GROUP BY department
ORDER BY avg_salary DESC;
```

---

## ğŸ“Š é¢è¯•é‡ç‚¹æ€»ç»“

### é«˜é¢‘é¢è¯•é¢˜

1. **Hadoop ç”Ÿæ€ç³»ç»Ÿ**
   - HDFS åŸç†å’Œæ¶æ„
   - MapReduce ç¼–ç¨‹æ¨¡å‹
   - YARN èµ„æºç®¡ç†

2. **Spark**
   - RDD å’Œ DataFrame
   - å®½çª„ä¾èµ–
   - æ•°æ®å€¾æ–œå¤„ç†
   - Spark SQL ä¼˜åŒ–

3. **Flink**
   - æµå¤„ç†å’Œæ‰¹å¤„ç†
   - æ—¶é—´è¯­ä¹‰
   - çª—å£æ“ä½œ
   - çŠ¶æ€ç®¡ç†

4. **Kafka**
   - Topic å’Œ Partition
   - æ¶ˆæ¯å­˜å‚¨æœºåˆ¶
   - æ¶ˆè´¹è€…ç»„
   - æ¶ˆæ¯é¡ºåºä¿è¯

5. **æ•°æ®å­˜å‚¨**
   - Hive åˆ†åŒºå’Œåˆ†æ¡¶
   - HBase è¡Œé”®è®¾è®¡
   - æ•°æ®æ¨¡å‹é€‰æ‹©

### å­¦ä¹ å»ºè®®

1. **ç†è®ºä¸å®è·µç»“åˆ**
   - ç†è§£åŸç†åï¼Œé€šè¿‡ä»£ç éªŒè¯
   - æ­å»ºå®éªŒç¯å¢ƒç»ƒä¹ 

2. **å¾ªåºæ¸è¿›**
   - å…ˆæŒæ¡åŸºç¡€ï¼Œå†æ·±å…¥é«˜çº§ç‰¹æ€§
   - æ¯ä¸ªçŸ¥è¯†ç‚¹éƒ½è¦æœ‰ä»£ç ç¤ºä¾‹

3. **æŒç»­ç»ƒä¹ **
   - å®šæœŸå›é¡¾çŸ¥è¯†ç‚¹
   - å‚ä¸å®é™…é¡¹ç›®å®è·µ
   - å…³æ³¨å¤§æ•°æ®æŠ€æœ¯æ›´æ–°

4. **é¢è¯•å‡†å¤‡**
   - å‡†å¤‡é¡¹ç›®ç»éªŒæè¿°
   - å‡†å¤‡æŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ
   - å‡†å¤‡æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹

---

## ğŸ”§ å·¥å…·æ¨è

### å¼€å‘å·¥å…·
- **IDE**ï¼šIntelliJ IDEAã€VS Code
- **æ„å»ºå·¥å…·**ï¼šMavenã€SBT
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šGit

### å¤§æ•°æ®å·¥å…·
- **Hadoop**ï¼šåˆ†å¸ƒå¼å­˜å‚¨å’Œè®¡ç®—
- **Spark**ï¼šå†…å­˜è®¡ç®—æ¡†æ¶
- **Flink**ï¼šæµå¤„ç†æ¡†æ¶
- **Kafka**ï¼šæ¶ˆæ¯é˜Ÿåˆ—
- **Hive**ï¼šæ•°æ®ä»“åº“
- **HBase**ï¼šNoSQL æ•°æ®åº“

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
1. **Hadoop å®˜æ–¹æ–‡æ¡£**ï¼šhttps://hadoop.apache.org/docs/
2. **Spark å®˜æ–¹æ–‡æ¡£**ï¼šhttps://spark.apache.org/docs/
3. **Flink å®˜æ–¹æ–‡æ¡£**ï¼šhttps://flink.apache.org/docs/
4. **Kafka å®˜æ–¹æ–‡æ¡£**ï¼šhttps://kafka.apache.org/documentation/

### åœ¨çº¿èµ„æº
1. **å¤§æ•°æ®æŠ€æœ¯åšå®¢**ï¼šå„ç§æŠ€æœ¯åšå®¢
2. **GitHub**ï¼šæœç´¢ç›¸å…³å¼€æºé¡¹ç›®æºç 

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ Hadoop ç”Ÿæ€ç³»ç»Ÿ
- [ ] æŒæ¡ Spark æ‰¹å¤„ç†
- [ ] ç†Ÿæ‚‰ Flink æµå¤„ç†
- [ ] ç†è§£ Kafka æ¶ˆæ¯é˜Ÿåˆ—
- [ ] æŒæ¡ Hive æ•°æ®ä»“åº“
- [ ] ç†Ÿæ‚‰ HBase NoSQL
- [ ] èƒ½å¤Ÿè®¾è®¡æ•°æ®ç®¡é“
- [ ] äº†è§£æ€§èƒ½ä¼˜åŒ–æ–¹æ³•

---

**æœ€åæ›´æ–°ï¼š2026-01-26**
