# çˆ¬è™«é«˜çº§å¼€å‘é¢è¯•å­¦ä¹ æŒ‡å—

## ğŸ“š é¡¹ç›®æ¦‚è¿°

æœ¬æŒ‡å—æä¾›äº†å®Œæ•´çš„çˆ¬è™«é«˜çº§å¼€å‘é¢è¯•å­¦ä¹ èµ„æºï¼ŒåŒ…æ‹¬æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€å®æˆ˜æ¡ˆä¾‹å’ŒéªŒè¯æ•°æ®ï¼Œå¸®åŠ©ä½ ç³»ç»ŸæŒæ¡çˆ¬è™«é«˜çº§å¼€å‘æŠ€æœ¯ï¼Œé¡ºåˆ©é€šè¿‡é¢è¯•ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
scrawel_interview/
â”œâ”€â”€ GUIDE.md                     # æœ¬æŒ‡å—æ–‡æ¡£ï¼ˆå¿«é€Ÿå…¥é—¨ï¼‰
â”œâ”€â”€ README.md                    # çˆ¬è™«é«˜çº§å¼€å‘çŸ¥è¯†ç‚¹æ€»è§ˆï¼ˆè¯¦ç»†æ–‡æ¡£ï¼‰
â”œâ”€â”€ cases/                       # å®æˆ˜æ¡ˆä¾‹ç›®å½•
â”‚   â”œâ”€â”€ basic_crawler.md        # æ¡ˆä¾‹1ï¼šåŸºç¡€çˆ¬è™«
â”‚   â”œâ”€â”€ data_parsing.md         # æ¡ˆä¾‹2ï¼šæ•°æ®è§£æ
â”‚   â”œâ”€â”€ anti_crawler.md         # æ¡ˆä¾‹3ï¼šåçˆ¬è™«ä¸åº”å¯¹
â”‚   â”œâ”€â”€ scrapy_framework.md     # æ¡ˆä¾‹4ï¼šScrapy æ¡†æ¶
â”‚   â”œâ”€â”€ distributed_crawler.md  # æ¡ˆä¾‹5ï¼šåˆ†å¸ƒå¼çˆ¬è™«
â”‚   â””â”€â”€ performance_optimization.md # æ¡ˆä¾‹6ï¼šæ€§èƒ½ä¼˜åŒ–
â”œâ”€â”€ data/                        # éªŒè¯æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ html_sample.html         # HTML ç¤ºä¾‹
â”‚   â”œâ”€â”€ api_response.json       # API å“åº”æ•°æ®
â”‚   â””â”€â”€ performance_test.txt    # æ€§èƒ½æµ‹è¯•æ•°æ®
â””â”€â”€ scripts/                     # ä»£ç ç¤ºä¾‹ç›®å½•
    â”œâ”€â”€ requests_demo.py        # requests åº“ç¤ºä¾‹
    â”œâ”€â”€ beautifulsoup_demo.py   # BeautifulSoup ç¤ºä¾‹
    â”œâ”€â”€ scrapy_spider.py        # Scrapy çˆ¬è™«ç¤ºä¾‹
    â””â”€â”€ selenium_demo.py        # Selenium ç¤ºä¾‹
```

---

## ğŸ¯ å­¦ä¹ è·¯å¾„

### é˜¶æ®µä¸€ï¼šçˆ¬è™«åŸºç¡€ï¼ˆ5-7å¤©ï¼‰
1. **HTTP åè®®åŸºç¡€**
   - HTTP è¯·æ±‚æ–¹æ³•
   - è¯·æ±‚å¤´å’Œå“åº”å¤´
   - Cookie å’Œ Session
   - çŠ¶æ€ç 

2. **è¯·æ±‚åº“ä½¿ç”¨**
   - requests åº“
   - urllib åº“
   - aiohttp å¼‚æ­¥è¯·æ±‚

3. **åŸºç¡€çˆ¬è™«å®ç°**
   - ç®€å•ç½‘é¡µçˆ¬å–
   - å›¾ç‰‡ä¸‹è½½
   - æ–‡ä»¶ä¸‹è½½

### é˜¶æ®µäºŒï¼šæ•°æ®è§£æï¼ˆ5-7å¤©ï¼‰
1. **HTML è§£æ**
   - BeautifulSoup
   - lxml
   - html.parser

2. **XPath å’Œ CSS é€‰æ‹©å™¨**
   - XPath è¯­æ³•
   - CSS é€‰æ‹©å™¨
   - é€‰æ‹©å™¨æ€§èƒ½å¯¹æ¯”

3. **æ­£åˆ™è¡¨è¾¾å¼**
   - æ­£åˆ™è¯­æ³•
   - å¸¸ç”¨æ¨¡å¼
   - æ€§èƒ½ä¼˜åŒ–

### é˜¶æ®µä¸‰ï¼šåçˆ¬è™«ä¸åº”å¯¹ï¼ˆ7-10å¤©ï¼‰
1. **å¸¸è§åçˆ¬è™«æœºåˆ¶**
   - User-Agent æ£€æµ‹
   - IP å°ç¦
   - éªŒè¯ç 
   - JavaScript æ¸²æŸ“

2. **åº”å¯¹ç­–ç•¥**
   - è¯·æ±‚å¤´ä¼ªè£…
   - ä»£ç†æ± 
   - éªŒè¯ç è¯†åˆ«
   - Selenium/Playwright

### é˜¶æ®µå››ï¼šScrapy æ¡†æ¶ï¼ˆ7-10å¤©ï¼‰
1. **Scrapy åŸºç¡€**
   - é¡¹ç›®ç»“æ„
   - Spider ç¼–å†™
   - Item å’Œ Pipeline
   - Middleware

2. **é«˜çº§ç‰¹æ€§**
   - åˆ†å¸ƒå¼çˆ¬è™«
   - å¢é‡çˆ¬å–
   - å»é‡ç­–ç•¥
   - æ•°æ®å­˜å‚¨

### é˜¶æ®µäº”ï¼šåˆ†å¸ƒå¼çˆ¬è™«ï¼ˆ5-7å¤©ï¼‰
1. **åˆ†å¸ƒå¼æ¶æ„**
   - Scrapy-Redis
   - æ¶ˆæ¯é˜Ÿåˆ—
   - ä»»åŠ¡è°ƒåº¦

2. **æ•°æ®å­˜å‚¨**
   - MySQL
   - MongoDB
   - Redis
   - æ–‡ä»¶å­˜å‚¨

### é˜¶æ®µå…­ï¼šæ€§èƒ½ä¼˜åŒ–ï¼ˆ5-7å¤©ï¼‰
1. **å¹¶å‘ä¼˜åŒ–**
   - å¤šçº¿ç¨‹çˆ¬è™«
   - å¼‚æ­¥çˆ¬è™«
   - åç¨‹æ± 

2. **èµ„æºä¼˜åŒ–**
   - è¿æ¥æ± 
   - è¯·æ±‚å»é‡
   - ç¼“å­˜ç­–ç•¥

---

## ğŸ“– æ ¸å¿ƒçŸ¥è¯†ç‚¹è¯¦è§£

### 1. åŸºç¡€çˆ¬è™«

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
æŒæ¡ HTTP åè®®å’Œè¯·æ±‚åº“æ˜¯çˆ¬è™«å¼€å‘çš„åŸºç¡€ã€‚

#### requests åº“ä½¿ç”¨

```python
# requests_demo.py
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# åŸºç¡€è¯·æ±‚
response = requests.get('https://www.example.com')
print(response.status_code)
print(response.text)

# å¸¦å‚æ•°çš„è¯·æ±‚
params = {'key': 'value'}
response = requests.get('https://www.example.com', params=params)

# POST è¯·æ±‚
data = {'username': 'user', 'password': 'pass'}
response = requests.post('https://www.example.com/login', data=data)

# è®¾ç½®è¯·æ±‚å¤´
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'zh-CN,zh;q=0.9'
}
response = requests.get('https://www.example.com', headers=headers)

# ä½¿ç”¨ Session
session = requests.Session()
session.headers.update(headers)
response = session.get('https://www.example.com')

# è®¾ç½®è¶…æ—¶å’Œé‡è¯•
session = requests.Session()
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504]
)
adapter = HTTPAdapter(max_retries=retry_strategy)
session.mount("http://", adapter)
session.mount("https://", adapter)
```

#### éªŒè¯æ•°æ®

**è¯·æ±‚æ€§èƒ½ï¼š**
```
å•çº¿ç¨‹è¯·æ±‚ï¼š100ä¸ªURLï¼Œè€—æ—¶ 50s
å¤šçº¿ç¨‹è¯·æ±‚ï¼ˆ10çº¿ç¨‹ï¼‰ï¼š100ä¸ªURLï¼Œè€—æ—¶ 6s
å¼‚æ­¥è¯·æ±‚ï¼š100ä¸ªURLï¼Œè€—æ—¶ 3s
```

---

### 2. æ•°æ®è§£æ

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
æ•°æ®è§£ææ˜¯çˆ¬è™«çš„æ ¸å¿ƒç¯èŠ‚ï¼ŒåŒ…æ‹¬ HTML è§£æã€XPathã€æ­£åˆ™è¡¨è¾¾å¼ç­‰ã€‚

#### BeautifulSoup ä½¿ç”¨

```python
# beautifulsoup_demo.py
from bs4 import BeautifulSoup
import requests

html = requests.get('https://www.example.com').text
soup = BeautifulSoup(html, 'lxml')

# æ ‡ç­¾æŸ¥æ‰¾
title = soup.find('title')
titles = soup.find_all('h1')

# CSS é€‰æ‹©å™¨
links = soup.select('a[href]')
divs = soup.select('.class-name')

# å±æ€§è·å–
link = soup.find('a')
href = link.get('href')
text = link.get_text()

# åµŒå¥—æŸ¥æ‰¾
div = soup.find('div', class_='content')
items = div.find_all('p')
```

#### XPath ä½¿ç”¨

```python
from lxml import etree

html = requests.get('https://www.example.com').text
tree = etree.HTML(html)

# XPath æŸ¥æ‰¾
titles = tree.xpath('//h1/text()')
links = tree.xpath('//a/@href')
divs = tree.xpath('//div[@class="content"]')
```

---

### 3. åçˆ¬è™«ä¸åº”å¯¹

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
ç†è§£åçˆ¬è™«æœºåˆ¶å¹¶æŒæ¡åº”å¯¹ç­–ç•¥æ˜¯é«˜çº§çˆ¬è™«å¼€å‘çš„æ ¸å¿ƒã€‚

#### åçˆ¬è™«ç­–ç•¥

**User-Agent ä¼ªè£…**
```python
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}
```

**ä»£ç†æ± **
```python
proxies = {
    'http': 'http://proxy.example.com:8080',
    'https': 'https://proxy.example.com:8080'
}
response = requests.get(url, proxies=proxies)
```

**Cookie å¤„ç†**
```python
session = requests.Session()
session.cookies.set('cookie_name', 'cookie_value')
response = session.get(url)
```

**Selenium å¤„ç† JavaScript**
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get('https://www.example.com')
element = driver.find_element(By.ID, 'element-id')
```

---

### 4. Scrapy æ¡†æ¶

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
Scrapy æ˜¯ Python æœ€å¼ºå¤§çš„çˆ¬è™«æ¡†æ¶ï¼ŒæŒæ¡å…¶ä½¿ç”¨æ˜¯é«˜çº§å¼€å‘çš„å¿…å¤‡æŠ€èƒ½ã€‚

#### Scrapy é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ scrapy.cfg
â””â”€â”€ project/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ items.py
    â”œâ”€â”€ middlewares.py
    â”œâ”€â”€ pipelines.py
    â”œâ”€â”€ settings.py
    â””â”€â”€ spiders/
        â””â”€â”€ example_spider.py
```

#### Spider ç¤ºä¾‹

```python
# scrapy_spider.py
import scrapy
from scrapy.crawler import CrawlerProcess

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['https://www.example.com']
    
    def parse(self, response):
        for item in response.css('div.item'):
            yield {
                'title': item.css('h2::text').get(),
                'link': item.css('a::attr(href)').get()
            }
        
        # ç¿»é¡µ
        next_page = response.css('a.next::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)
```

---

### 5. åˆ†å¸ƒå¼çˆ¬è™«

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
åˆ†å¸ƒå¼çˆ¬è™«å¯ä»¥å¤§å¹…æå‡çˆ¬å–æ•ˆç‡ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é‡‡é›†ã€‚

#### Scrapy-Redis

```python
# settings.py
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
REDIS_URL = 'redis://localhost:6379'

# spider.py
from scrapy_redis.spiders import RedisSpider

class DistributedSpider(RedisSpider):
    name = 'distributed'
    redis_key = 'spider:start_urls'
    
    def parse(self, response):
        # è§£æé€»è¾‘
        pass
```

---

### 6. æ€§èƒ½ä¼˜åŒ–

#### çŸ¥è¯†ç‚¹æ¦‚è¿°
æ€§èƒ½ä¼˜åŒ–æ˜¯æå‡çˆ¬è™«æ•ˆç‡çš„å…³é”®ã€‚

#### å¹¶å‘ä¼˜åŒ–

```python
# å¼‚æ­¥çˆ¬è™«
import asyncio
import aiohttp

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return results

results = asyncio.run(main())
```

---

## ğŸ“Š é¢è¯•é‡ç‚¹æ€»ç»“

### é«˜é¢‘é¢è¯•é¢˜

1. **çˆ¬è™«åŸºç¡€**
   - HTTP åè®®
   - requests åº“ä½¿ç”¨
   - Cookie å’Œ Session

2. **æ•°æ®è§£æ**
   - BeautifulSoup vs lxml
   - XPath vs CSS é€‰æ‹©å™¨
   - æ­£åˆ™è¡¨è¾¾å¼

3. **åçˆ¬è™«**
   - å¸¸è§åçˆ¬è™«æœºåˆ¶
   - åº”å¯¹ç­–ç•¥
   - éªŒè¯ç è¯†åˆ«

4. **Scrapy æ¡†æ¶**
   - é¡¹ç›®ç»“æ„
   - Spider ç¼–å†™
   - Pipeline å’Œ Middleware

5. **åˆ†å¸ƒå¼çˆ¬è™«**
   - Scrapy-Redis
   - ä»»åŠ¡è°ƒåº¦
   - æ•°æ®å»é‡

6. **æ€§èƒ½ä¼˜åŒ–**
   - å¹¶å‘ä¼˜åŒ–
   - èµ„æºä¼˜åŒ–
   - ç¼“å­˜ç­–ç•¥

### å­¦ä¹ å»ºè®®

1. **ç†è®ºä¸å®è·µç»“åˆ**
   - ç†è§£åŸç†åï¼Œé€šè¿‡ä»£ç éªŒè¯
   - å®é™…çˆ¬å–ç½‘ç«™ç»ƒä¹ 

2. **å¾ªåºæ¸è¿›**
   - å…ˆæŒæ¡åŸºç¡€ï¼Œå†æ·±å…¥æ¡†æ¶
   - æ¯ä¸ªçŸ¥è¯†ç‚¹éƒ½è¦æœ‰ä»£ç ç¤ºä¾‹

3. **æŒç»­ç»ƒä¹ **
   - å®šæœŸå›é¡¾çŸ¥è¯†ç‚¹
   - å‚ä¸å®é™…é¡¹ç›®å®è·µ
   - å…³æ³¨åçˆ¬è™«æŠ€æœ¯

4. **é¢è¯•å‡†å¤‡**
   - å‡†å¤‡é¡¹ç›®ç»éªŒæè¿°
   - å‡†å¤‡æŠ€æœ¯éš¾ç‚¹å’Œè§£å†³æ–¹æ¡ˆ
   - å‡†å¤‡æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹

---

## ğŸ”§ å·¥å…·æ¨è

### å¼€å‘å·¥å…·
- **IDE**ï¼šPyCharmã€VS Code
- **æµè§ˆå™¨**ï¼šChrome DevTools
- **æŠ“åŒ…å·¥å…·**ï¼šFiddlerã€Charles

### çˆ¬è™«åº“
- **requests**ï¼šHTTP è¯·æ±‚
- **BeautifulSoup**ï¼šHTML è§£æ
- **Scrapy**ï¼šçˆ¬è™«æ¡†æ¶
- **Selenium**ï¼šæµè§ˆå™¨è‡ªåŠ¨åŒ–

### æ•°æ®å­˜å‚¨
- **MySQL**ï¼šå…³ç³»å‹æ•°æ®åº“
- **MongoDB**ï¼šæ–‡æ¡£æ•°æ®åº“
- **Redis**ï¼šç¼“å­˜å’Œé˜Ÿåˆ—

---

## ğŸ“š å‚è€ƒèµ„æº

### ä¹¦ç±æ¨è
1. ã€ŠPython ç½‘ç»œçˆ¬è™«ä»å…¥é—¨åˆ°å®è·µã€‹
2. ã€ŠScrapy ç½‘ç»œçˆ¬è™«å®æˆ˜ã€‹
3. ã€ŠPython çˆ¬è™«å¼€å‘ä¸é¡¹ç›®å®æˆ˜ã€‹

### åœ¨çº¿èµ„æº
1. **Scrapy å®˜æ–¹æ–‡æ¡£**ï¼šhttps://docs.scrapy.org/
2. **BeautifulSoup æ–‡æ¡£**ï¼šhttps://www.crummy.com/software/BeautifulSoup/
3. **requests æ–‡æ¡£**ï¼šhttps://requests.readthedocs.io/

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£ HTTP åè®®å’Œè¯·æ±‚åº“
- [ ] æŒæ¡æ•°æ®è§£ææ–¹æ³•ï¼ˆBeautifulSoupã€XPathï¼‰
- [ ] äº†è§£åçˆ¬è™«æœºåˆ¶å’Œåº”å¯¹ç­–ç•¥
- [ ] ç†Ÿæ‚‰ Scrapy æ¡†æ¶ä½¿ç”¨
- [ ] æŒæ¡åˆ†å¸ƒå¼çˆ¬è™«å®ç°
- [ ] èƒ½å¤Ÿè¿›è¡Œæ€§èƒ½ä¼˜åŒ–
- [ ] å…·å¤‡é¡¹ç›®å®æˆ˜ç»éªŒ
- [ ] äº†è§£æ³•å¾‹æ³•è§„å’Œé“å¾·è§„èŒƒ

---

**æœ€åæ›´æ–°ï¼š2026-01-26**
