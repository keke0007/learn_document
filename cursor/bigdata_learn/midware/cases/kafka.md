# Kafka é«˜æ€§èƒ½åŸç†ä¸é«˜çº§åº”ç”¨æ¡ˆä¾‹ï¼ˆæ·±å…¥ç‰ˆï¼‰

## æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ¡ˆä¾‹æ·±å…¥ Kafka é«˜æ€§èƒ½åŸç†ã€æ•°æ®ç»“æ„ç»„åˆæœºåˆ¶ã€é«˜çº§åº”ç”¨åœºæ™¯ã€‚**é‡ç‚¹ï¼šé¡ºåºå†™ç£ç›˜ã€åˆ†æ®µå­˜å‚¨ã€é›¶æ‹·è´ã€ISR å‰¯æœ¬æœºåˆ¶ã€çœŸå®ä¸šåŠ¡åœºæ™¯è®¾è®¡ã€‚**

---

## ğŸš€ é«˜æ€§èƒ½åŸç†

### 1. é¡ºåºå†™ç£ç›˜ä¸åˆ†æ®µå­˜å‚¨

**é¡ºåºå†™ç£ç›˜ä¼˜åŠ¿ï¼š**
- **æœºæ¢°ç¡¬ç›˜**ï¼šé¡ºåºå†™ 100MB/sï¼Œéšæœºå†™ < 1MB/sï¼ˆ100å€å·®è·ï¼‰
- **SSD**ï¼šé¡ºåºå†™ 500MB/sï¼Œéšæœºå†™ 50MB/sï¼ˆ10å€å·®è·ï¼‰
- Kafka é‡‡ç”¨ append-only é¡ºåºå†™ï¼Œå……åˆ†åˆ©ç”¨ç£ç›˜å¸¦å®½

**åˆ†æ®µå­˜å‚¨ï¼ˆSegmentï¼‰æœºåˆ¶ï¼š**
```
Topic Partition ç›®å½•ç»“æ„ï¼š
topic-0/
  â”œâ”€ 00000000000000000000.log    # Segment 0ï¼ˆ1GBï¼‰
  â”œâ”€ 00000000000000000000.index  # åç§»é‡ç´¢å¼•
  â”œâ”€ 00000000000000000000.timeindex  # æ—¶é—´ç´¢å¼•
  â”œâ”€ 000000000001073741824.log    # Segment 1ï¼ˆ1GBï¼‰
  â”œâ”€ 000000000001073741824.index
  â””â”€ ...
```

**åˆ†æ®µç­–ç•¥ï¼š**
- **å¤§å°ç­–ç•¥**ï¼šæ¯ä¸ª segment é»˜è®¤ 1GBï¼ˆ`log.segment.bytes`ï¼‰
- **æ—¶é—´ç­–ç•¥**ï¼šè¶…è¿‡ 7 å¤©è‡ªåŠ¨æ»šåŠ¨ï¼ˆ`log.roll.hours`ï¼‰
- **ç´¢å¼•ç­–ç•¥**ï¼šæ¯å†™å…¥ 4KB æ•°æ®ï¼Œæ›´æ–°ä¸€æ¬¡ç´¢å¼•ï¼ˆ`log.index.interval.bytes`ï¼‰

**ç´¢å¼•æ–‡ä»¶ç»“æ„ï¼š**
```
Offset Indexï¼ˆ.indexï¼‰ï¼š
+------------------+
| relativeOffset   | 4 bytesï¼ˆç›¸å¯¹åç§»é‡ï¼‰
| position         | 4 bytesï¼ˆç‰©ç†ä½ç½®ï¼‰
+------------------+

Time Indexï¼ˆ.timeindexï¼‰ï¼š
+------------------+
| timestamp        | 8 bytesï¼ˆæ—¶é—´æˆ³ï¼‰
| relativeOffset   | 4 bytesï¼ˆç›¸å¯¹åç§»é‡ï¼‰
+------------------+
```

**æŸ¥æ‰¾æœºåˆ¶ï¼š**
- **æŒ‰åç§»é‡æŸ¥æ‰¾**ï¼šäºŒåˆ†æŸ¥æ‰¾ `.index` â†’ æ‰¾åˆ°æœ€è¿‘çš„ä½ç½® â†’ é¡ºåºæ‰«æ `.log`
- **æŒ‰æ—¶é—´æŸ¥æ‰¾**ï¼šäºŒåˆ†æŸ¥æ‰¾ `.timeindex` â†’ æ‰¾åˆ°åç§»é‡ â†’ æŒ‰åç§»é‡æŸ¥æ‰¾

---

### 2. é›¶æ‹·è´ï¼ˆZero Copyï¼‰æœºåˆ¶

**ä¼ ç»Ÿæ–‡ä»¶ä¼ è¾“ï¼ˆ4æ¬¡æ‹·è´ï¼‰ï¼š**
```
ç£ç›˜æ–‡ä»¶
  -> å†…æ ¸ç¼“å†²åŒºï¼ˆDMAï¼‰
    -> ç”¨æˆ·ç¼“å†²åŒºï¼ˆCPU æ‹·è´ï¼‰
      -> Socket ç¼“å†²åŒºï¼ˆCPU æ‹·è´ï¼‰
        -> ç½‘å¡ï¼ˆDMAï¼‰
```

**Kafka é›¶æ‹·è´ï¼ˆ2æ¬¡æ‹·è´ï¼‰ï¼š**
```
ç£ç›˜æ–‡ä»¶
  -> å†…æ ¸ç¼“å†²åŒºï¼ˆDMAï¼‰
    -> ç½‘å¡ï¼ˆDMAï¼Œsendfile ç³»ç»Ÿè°ƒç”¨ï¼‰
```

**æ€§èƒ½æå‡ï¼š**
- **å‡å°‘ CPU æ‹·è´**ï¼šä» 2 æ¬¡å‡å°‘åˆ° 0 æ¬¡
- **å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢**ï¼šä» 4 æ¬¡å‡å°‘åˆ° 2 æ¬¡
- **ååé‡æå‡**ï¼š2-3 å€

**å®ç°æ–¹å¼ï¼š**
- **Linux**ï¼š`sendfile()` ç³»ç»Ÿè°ƒç”¨
- **Java**ï¼š`FileChannel.transferTo()`

---

### 3. æ‰¹æ¬¡ç´¯ç§¯ä¸å‹ç¼©

**æ‰¹æ¬¡ç´¯ç§¯æœºåˆ¶ï¼š**
- **RecordAccumulator**ï¼šæŒ‰åˆ†åŒºç´¯ç§¯æ¶ˆæ¯ï¼Œå½¢æˆæ‰¹æ¬¡
- **è§¦å‘æ¡ä»¶**ï¼š
  - æ‰¹æ¬¡å¤§å°è¾¾åˆ° `batch.size`ï¼ˆé»˜è®¤ 16KBï¼‰
  - ç­‰å¾…æ—¶é—´è¾¾åˆ° `linger.ms`ï¼ˆé»˜è®¤ 0msï¼Œå¯è®¾ç½® 10-100msï¼‰
  - ç¼“å†²åŒºæ»¡ï¼ˆ`buffer.memory`ï¼Œé»˜è®¤ 32MBï¼‰

**å‹ç¼©ç®—æ³•å¯¹æ¯”ï¼š**
| ç®—æ³• | å‹ç¼©æ¯” | CPU å¼€é”€ | é€‚ç”¨åœºæ™¯ |
|-----|--------|----------|---------|
| none | 1:1 | 0 | æ€§èƒ½ä¼˜å…ˆ |
| gzip | 3:1 | é«˜ | é«˜å‹ç¼©æ¯” |
| snappy | 2:1 | ä¸­ | å¹³è¡¡æ€§èƒ½ |
| lz4 | 2:1 | ä½ | ä½å»¶è¿Ÿ |
| zstd | 3:1 | ä¸­ | æœ€ä½³å‹ç¼©æ¯” |

**å‹ç¼©ä¼˜åŠ¿ï¼š**
- **å‡å°‘ç½‘ç»œä¼ è¾“**ï¼šå‹ç¼©åæ•°æ®é‡å‡å°‘ 50-70%
- **å‡å°‘ç£ç›˜ IO**ï¼šå†™å…¥æ•°æ®é‡å‡å°‘
- **æé«˜ååé‡**ï¼šç½‘ç»œå¸¦å®½åˆ©ç”¨ç‡æå‡

---

### 4. ISR å‰¯æœ¬æœºåˆ¶ï¼ˆIn-Sync Replicasï¼‰

**ISR å®šä¹‰ï¼š**
- **åŒæ­¥å‰¯æœ¬**ï¼šä¸ Leader å‰¯æœ¬æ•°æ®åŒæ­¥çš„å‰¯æœ¬é›†åˆ
- **åŒæ­¥æ¡ä»¶**ï¼š
  - å‰¯æœ¬ä¸ Leader çš„å»¶è¿Ÿ < `replica.lag.time.max.ms`ï¼ˆé»˜è®¤ 10sï¼‰
  - å‰¯æœ¬ä¸ Leader çš„åç§»é‡å·® < `replica.lag.max.messages`ï¼ˆå·²å¼ƒç”¨ï¼‰

**ISR åŠ¨æ€ç»´æŠ¤ï¼š**
```
Leader å†™å…¥æ¶ˆæ¯
  -> åŒæ­¥åˆ° Follower
    -> Follower ç¡®è®¤
      -> æ›´æ–° ISR
        -> å¦‚æœå»¶è¿Ÿ > 10sï¼Œä» ISR ç§»é™¤
        -> å¦‚æœå»¶è¿Ÿ < 10sï¼ŒåŠ å…¥ ISR
```

**å¯é æ€§ä¿è¯ï¼š**
- **acks=all**ï¼šç­‰å¾…æ‰€æœ‰ ISR å‰¯æœ¬ç¡®è®¤
- **min.insync.replicas=2**ï¼šè‡³å°‘ 2 ä¸ª ISR å‰¯æœ¬ï¼ˆåŒ…æ‹¬ Leaderï¼‰
- **æ•…éšœè½¬ç§»**ï¼šLeader å´©æºƒï¼Œä» ISR ä¸­é€‰æ‹©æ–° Leader

**æ€§èƒ½æƒè¡¡ï¼š**
- ISR å‰¯æœ¬å¤š â†’ å¯é æ€§é«˜ï¼Œä½†å†™å…¥å»¶è¿Ÿé«˜
- ISR å‰¯æœ¬å°‘ â†’ å†™å…¥å»¶è¿Ÿä½ï¼Œä½†å¯é æ€§ä½

---

### 5. æ¶ˆè´¹è€…ç»„ä¸è´Ÿè½½å‡è¡¡

**æ¶ˆè´¹è€…ç»„æœºåˆ¶ï¼š**
- **ç»„å†…è´Ÿè½½å‡è¡¡**ï¼šä¸€ä¸ªåˆ†åŒºåªèƒ½è¢«ç»„å†…ä¸€ä¸ªæ¶ˆè´¹è€…æ¶ˆè´¹
- **Rebalance è§¦å‘æ¡ä»¶**ï¼š
  - æ¶ˆè´¹è€…åŠ å…¥/ç¦»å¼€
  - åˆ†åŒºæ•°å˜åŒ–
  - å¿ƒè·³è¶…æ—¶ï¼ˆ`session.timeout.ms`ï¼Œé»˜è®¤ 10sï¼‰

**åˆ†åŒºåˆ†é…ç­–ç•¥ï¼š**
- **Range**ï¼šæŒ‰åˆ†åŒºèŒƒå›´åˆ†é…ï¼ˆå¯èƒ½ä¸å‡åŒ€ï¼‰
- **RoundRobin**ï¼šè½®è¯¢åˆ†é…ï¼ˆå‡åŒ€ï¼Œä½†éœ€è¦æ‰€æœ‰æ¶ˆè´¹è€…è®¢é˜…ç›¸åŒä¸»é¢˜ï¼‰
- **Sticky**ï¼šç²˜æ€§åˆ†é…ï¼ˆå‡å°‘ Rebalance æ—¶çš„åˆ†åŒºè¿ç§»ï¼‰
- **Cooperative Sticky**ï¼šåä½œå¼ç²˜æ€§ï¼ˆå¢é‡ Rebalanceï¼‰

**Rebalance è¿‡ç¨‹ï¼š**
```
æ¶ˆè´¹è€…åŠ å…¥/ç¦»å¼€
  -> Coordinator æ£€æµ‹å˜åŒ–
    -> è§¦å‘ Rebalance
      -> æ‰€æœ‰æ¶ˆè´¹è€…åœæ­¢æ¶ˆè´¹
        -> é‡æ–°åˆ†é…åˆ†åŒº
          -> æ¶ˆè´¹è€…æ¢å¤æ¶ˆè´¹
```

**æ€§èƒ½å½±å“ï¼š**
- Rebalance æœŸé—´ï¼Œæ¶ˆè´¹è€…åœæ­¢æ¶ˆè´¹ï¼ˆStop The Worldï¼‰
- é¢‘ç¹ Rebalance â†’ æ¶ˆè´¹å»¶è¿Ÿé«˜
- ä¼˜åŒ–ï¼šå¢åŠ  `session.timeout.ms`ã€å‡å°‘æ¶ˆè´¹è€…æ•°é‡å˜åŒ–

---

## ğŸ”§ æ•°æ®ç»“æ„ç»„åˆåŠŸèƒ½

### ç»„åˆ1ï¼šé¡ºåºæ¶ˆæ¯ + åˆ†åŒºè·¯ç”±

**æ•°æ®ç»“æ„ç»„åˆï¼š**
- **åˆ†åŒºï¼ˆPartitionï¼‰**ï¼šä¿è¯åŒä¸€åˆ†åŒºå†…æ¶ˆæ¯é¡ºåº
- **Key è·¯ç”±**ï¼šç›¸åŒ key çš„æ¶ˆæ¯è·¯ç”±åˆ°åŒä¸€åˆ†åŒº
- **åç§»é‡ï¼ˆOffsetï¼‰**ï¼šåˆ†åŒºå†…æ¶ˆæ¯çš„å”¯ä¸€æ ‡è¯†

**é¡ºåºæ¶ˆæ¯è®¾è®¡ï¼š**
```python
# ä½¿ç”¨ key ä¿è¯åˆ†åŒºé¡ºåº
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    key_serializer=lambda k: k.encode('utf-8'),
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# ç›¸åŒ user_id çš„æ¶ˆæ¯ä¼šè·¯ç”±åˆ°åŒä¸€åˆ†åŒº
for event in user_events:
    producer.send(
        'user-events',
        key=event['user_id'],  # å…³é”®ï¼šä½¿ç”¨ key è·¯ç”±
        value=event
    )
```

**åˆ†åŒºè·¯ç”±ç®—æ³•ï¼š**
```python
# åˆ†åŒºé€‰æ‹©ï¼ˆç®€åŒ–ç‰ˆï¼‰
def partition(key, num_partitions):
    if key is None:
        return round_robin()  # è½®è¯¢
    else:
        return hash(key) % num_partitions  # å“ˆå¸Œ
```

---

### ç»„åˆ2ï¼šæ‰¹é‡æ¶ˆè´¹ + å¹‚ç­‰å¤„ç†

**æ•°æ®ç»“æ„ç»„åˆï¼š**
- **æ‰¹æ¬¡æ‹‰å–**ï¼š`max.poll.records`ï¼ˆé»˜è®¤ 500ï¼‰æ‰¹é‡æ‹‰å–
- **åç§»é‡ç®¡ç†**ï¼šæ‰‹åŠ¨æäº¤åç§»é‡ï¼Œä¿è¯å¹‚ç­‰æ€§
- **å¹‚ç­‰é”®**ï¼šæ¶ˆæ¯ä¸­çš„å”¯ä¸€æ ‡è¯†ï¼ˆå¦‚è®¢å•IDï¼‰

**æ‰¹é‡æ¶ˆè´¹è®¾è®¡ï¼š**
```python
consumer = KafkaConsumer(
    'order-events',
    bootstrap_servers=['localhost:9092'],
    group_id='order-processor',
    enable_auto_commit=False,  # æ‰‹åŠ¨æäº¤
    max_poll_records=500,  # æ‰¹é‡æ‹‰å–
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

while True:
    records = consumer.poll(timeout_ms=1000)
    
    # æ‰¹é‡å¤„ç†
    processed = []
    for topic_partition, messages in records.items():
        for message in messages:
            # å¹‚ç­‰æ€§æ£€æŸ¥
            order_id = message.value['order_id']
            if not is_processed(order_id):  # Redis/DB å»é‡
                process_order(message.value)
                mark_as_processed(order_id)
                processed.append(message)
    
    # æ‰¹é‡æäº¤åç§»é‡
    if processed:
        consumer.commit()
```

---

### ç»„åˆ3ï¼šäº‹åŠ¡æ¶ˆæ¯ + ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰

**æ•°æ®ç»“æ„ç»„åˆï¼š**
- **äº‹åŠ¡ ID**ï¼š`transactional.id` å”¯ä¸€æ ‡è¯†äº‹åŠ¡ç”Ÿäº§è€…
- **äº‹åŠ¡çŠ¶æ€**ï¼š`__transaction_state` å†…éƒ¨ä¸»é¢˜å­˜å‚¨äº‹åŠ¡çŠ¶æ€
- **PIDï¼ˆProducer IDï¼‰**ï¼šå¹‚ç­‰æ€§ä¿è¯

**äº‹åŠ¡æ¶ˆæ¯è®¾è®¡ï¼š**
```python
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    transactional_id='order-service-producer',
    enable_idempotence=True,  # å¯ç”¨å¹‚ç­‰æ€§
    acks='all',
    retries=3
)

# åˆå§‹åŒ–äº‹åŠ¡
producer.init_transactions()

try:
    # å¼€å§‹äº‹åŠ¡
    producer.begin_transaction()
    
    # å‘é€å¤šæ¡æ¶ˆæ¯ï¼ˆåŸå­æ€§ï¼‰
    producer.send('order-created', order_data)
    producer.send('inventory-updated', inventory_data)
    producer.send('payment-processed', payment_data)
    
    # æäº¤äº‹åŠ¡
    producer.commit_transaction()
except Exception as e:
    # å›æ»šäº‹åŠ¡
    producer.abort_transaction()
    raise e
```

**ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰ï¼ˆEOSï¼‰ä¿è¯ï¼š**
- **å¹‚ç­‰æ€§**ï¼šç›¸åŒ PID + åºåˆ—å·çš„æ¶ˆæ¯åªå†™å…¥ä¸€æ¬¡
- **äº‹åŠ¡æ€§**ï¼šäº‹åŠ¡å†…çš„æ¶ˆæ¯è¦ä¹ˆå…¨éƒ¨æˆåŠŸï¼Œè¦ä¹ˆå…¨éƒ¨å¤±è´¥
- **æ¶ˆè´¹è€…äº‹åŠ¡**ï¼šè¯»å–-å¤„ç†-å†™å…¥çš„åŸå­æ€§ï¼ˆKafka Streamsï¼‰

---

## ğŸ’¼ é«˜çº§åº”ç”¨åœºæ™¯æ¡ˆä¾‹

### åœºæ™¯1ï¼šè®¢å•ç³»ç»Ÿäº‹ä»¶é©±åŠ¨æ¶æ„

**ä¸šåŠ¡éœ€æ±‚ï¼š**
- è®¢å•åˆ›å»ºåï¼Œè§¦å‘åº“å­˜æ‰£å‡ã€æ”¯ä»˜å¤„ç†ã€ç‰©æµé€šçŸ¥
- ä¿è¯æœ€ç»ˆä¸€è‡´æ€§ï¼ˆè®¢å•çŠ¶æ€æœ€ç»ˆä¸€è‡´ï¼‰
- æ”¯æŒè®¢å•çŠ¶æ€æŸ¥è¯¢å’Œè¡¥å¿æœºåˆ¶

**Topic è®¾è®¡ï¼š**
```
order-eventsï¼ˆè®¢å•äº‹ä»¶ï¼‰
  â”œâ”€ order-createdï¼ˆè®¢å•åˆ›å»ºï¼‰
  â”œâ”€ order-paidï¼ˆè®¢å•æ”¯ä»˜ï¼‰
  â”œâ”€ order-shippedï¼ˆè®¢å•å‘è´§ï¼‰
  â””â”€ order-completedï¼ˆè®¢å•å®Œæˆï¼‰

inventory-eventsï¼ˆåº“å­˜äº‹ä»¶ï¼‰
  â”œâ”€ inventory-reservedï¼ˆåº“å­˜é¢„ç•™ï¼‰
  â”œâ”€ inventory-deductedï¼ˆåº“å­˜æ‰£å‡ï¼‰
  â””â”€ inventory-releasedï¼ˆåº“å­˜é‡Šæ”¾ï¼‰

payment-eventsï¼ˆæ”¯ä»˜äº‹ä»¶ï¼‰
  â”œâ”€ payment-initiatedï¼ˆæ”¯ä»˜å‘èµ·ï¼‰
  â”œâ”€ payment-successï¼ˆæ”¯ä»˜æˆåŠŸï¼‰
  â””â”€ payment-failedï¼ˆæ”¯ä»˜å¤±è´¥ï¼‰
```

**åˆ†åŒºè®¾è®¡ï¼š**
```python
# æŒ‰è®¢å•IDåˆ†åŒºï¼Œä¿è¯åŒä¸€è®¢å•çš„äº‹ä»¶é¡ºåº
producer.send(
    'order-events',
    key=order_id,  # å…³é”®ï¼šä½¿ç”¨è®¢å•IDä½œä¸ºkey
    value={
        'event_type': 'order-created',
        'order_id': order_id,
        'user_id': user_id,
        'items': items,
        'total_amount': total_amount,
        'timestamp': datetime.now().isoformat()
    }
)
```

**æ¶ˆè´¹è€…è®¾è®¡ï¼š**
```python
# è®¢å•æœåŠ¡æ¶ˆè´¹è€…
order_consumer = KafkaConsumer(
    'order-events',
    group_id='order-service',
    bootstrap_servers=['localhost:9092']
)

# åº“å­˜æœåŠ¡æ¶ˆè´¹è€…
inventory_consumer = KafkaConsumer(
    'order-events',
    group_id='inventory-service',
    bootstrap_servers=['localhost:9092']
)

# æ”¯ä»˜æœåŠ¡æ¶ˆè´¹è€…
payment_consumer = KafkaConsumer(
    'order-events',
    group_id='payment-service',
    bootstrap_servers=['localhost:9092']
)

# æ¯ä¸ªæœåŠ¡ç‹¬ç«‹æ¶ˆè´¹ï¼Œå®ç°è§£è€¦
```

**æœ€ç»ˆä¸€è‡´æ€§ä¿è¯ï¼š**
- **Saga æ¨¡å¼**ï¼šæ¯ä¸ªæœåŠ¡å¤„ç†äº‹ä»¶åï¼Œå‘é€ä¸‹ä¸€ä¸ªäº‹ä»¶
- **è¡¥å¿æœºåˆ¶**ï¼šå¤±è´¥æ—¶å‘é€è¡¥å¿äº‹ä»¶ï¼ˆå¦‚è®¢å•å–æ¶ˆ â†’ åº“å­˜é‡Šæ”¾ï¼‰
- **å¹‚ç­‰æ€§**ï¼šæ¯ä¸ªæœåŠ¡å®ç°å¹‚ç­‰å¤„ç†ï¼ˆåŸºäºè®¢å•IDå»é‡ï¼‰

**æ€§èƒ½æŒ‡æ ‡ï¼š**
- **ååé‡**ï¼š10ä¸‡è®¢å•/åˆ†é’Ÿï¼ˆå•åˆ†åŒºï¼‰
- **å»¶è¿Ÿ**ï¼šP99 å»¶è¿Ÿ < 100msï¼ˆç«¯åˆ°ç«¯ï¼‰
- **å¯é æ€§**ï¼šæ¶ˆæ¯ä¸ä¸¢å¤±ï¼ˆ`acks=all`ï¼Œ`min.insync.replicas=2`ï¼‰

---

### åœºæ™¯2ï¼šå®æ—¶æ•°æ®ç®¡é“ï¼ˆETLï¼‰

**ä¸šåŠ¡éœ€æ±‚ï¼š**
- ä»å¤šä¸ªæ•°æ®æºï¼ˆæ•°æ®åº“ã€æ—¥å¿—ã€APIï¼‰å®æ—¶é‡‡é›†æ•°æ®
- æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€èšåˆ
- å†™å…¥ç›®æ ‡ç³»ç»Ÿï¼ˆæ•°æ®ä»“åº“ã€æœç´¢å¼•æ“ã€ç¼“å­˜ï¼‰

**æ•°æ®ç®¡é“æ¶æ„ï¼š**
```
æ•°æ®æº
  â”œâ”€ MySQLï¼ˆCDCï¼‰-> kafka-connector -> raw-data-topic
  â”œâ”€ æ—¥å¿—æ–‡ä»¶ -> Filebeat -> log-events-topic
  â””â”€ API -> åº”ç”¨ -> api-events-topic
        â†“
   Kafkaï¼ˆç»Ÿä¸€æ¶ˆæ¯æ€»çº¿ï¼‰
        â†“
   æµå¤„ç†ï¼ˆKafka Streams / Flinkï¼‰
        â†“
   ç›®æ ‡ç³»ç»Ÿ
  â”œâ”€ Elasticsearchï¼ˆæœç´¢ï¼‰
  â”œâ”€ ClickHouseï¼ˆOLAPï¼‰
  â””â”€ Redisï¼ˆç¼“å­˜ï¼‰
```

**Topic è®¾è®¡ï¼š**
```json
{
  "topics": [
    {
      "name": "raw-data-topic",
      "partitions": 10,
      "replication-factor": 3,
      "retention": "7d"
    },
    {
      "name": "cleaned-data-topic",
      "partitions": 10,
      "replication-factor": 3,
      "retention": "30d"
    },
    {
      "name": "aggregated-data-topic",
      "partitions": 5,
      "replication-factor": 3,
      "retention": "90d"
    }
  ]
}
```

**æµå¤„ç†è®¾è®¡ï¼ˆKafka Streamsï¼‰ï¼š**
```java
// æ•°æ®æ¸…æ´—å’Œè½¬æ¢
KStream<String, RawEvent> rawStream = builder.stream("raw-data-topic");

KStream<String, CleanedEvent> cleanedStream = rawStream
    .filter((key, value) -> value != null && value.isValid())
    .mapValues(value -> {
        // æ•°æ®æ¸…æ´—
        return cleanData(value);
    })
    .to("cleaned-data-topic");

// æ•°æ®èšåˆ
KStream<String, CleanedEvent> cleanedStream = builder.stream("cleaned-data-topic");

KTable<String, AggregatedData> aggregatedTable = cleanedStream
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
    .aggregate(
        () -> new AggregatedData(),
        (key, value, aggregate) -> aggregate.add(value),
        Materialized.as("aggregated-store")
    );

aggregatedTable.toStream().to("aggregated-data-topic");
```

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- **æ‰¹é‡å†™å…¥**ï¼šä½¿ç”¨ Kafka Connect æ‰¹é‡å†™å…¥ï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
- **å‹ç¼©**ï¼šä½¿ç”¨ Snappy å‹ç¼©ï¼Œå‡å°‘å­˜å‚¨å’Œç½‘ç»œä¼ è¾“
- **åˆ†åŒºç­–ç•¥**ï¼šæŒ‰ä¸šåŠ¡é”®åˆ†åŒºï¼Œä¿è¯ç›¸åŒä¸šåŠ¡çš„æ•°æ®åœ¨åŒä¸€åˆ†åŒº

**éªŒè¯æ•°æ®ï¼š**
- **ååé‡**ï¼š100ä¸‡æ¡/åˆ†é’Ÿï¼ˆå•åˆ†åŒºï¼‰
- **å»¶è¿Ÿ**ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿ < 5sï¼ˆåŒ…å«æµå¤„ç†ï¼‰
- **å¯é æ€§**ï¼šæ¶ˆæ¯ä¸ä¸¢å¤±ï¼Œæ”¯æŒé‡æ”¾

---

### åœºæ™¯3ï¼šæ—¥å¿—èšåˆä¸ç›‘æ§å‘Šè­¦

**ä¸šåŠ¡éœ€æ±‚ï¼š**
- æ”¶é›†æ‰€æœ‰æœåŠ¡çš„æ—¥å¿—ï¼ˆå¾®æœåŠ¡æ¶æ„ï¼‰
- å®æ—¶ç»Ÿè®¡é”™è¯¯ç‡ã€å“åº”æ—¶é—´ã€QPS
- å¼‚å¸¸å‘Šè­¦ï¼ˆé”™è¯¯ç‡çªå¢ã€å“åº”æ—¶é—´è¶…é˜ˆå€¼ï¼‰

**æ—¥å¿—æ”¶é›†æ¶æ„ï¼š**
```
æœåŠ¡èŠ‚ç‚¹
  â”œâ”€ Service A -> Filebeat -> log-topic-partition-0
  â”œâ”€ Service B -> Filebeat -> log-topic-partition-1
  â””â”€ Service C -> Filebeat -> log-topic-partition-2
        â†“
    Kafkaï¼ˆæŒ‰æœåŠ¡åˆ†åŒºï¼‰
        â†“
   æ—¥å¿—å¤„ç†æœåŠ¡
  â”œâ”€ é”™è¯¯æ—¥å¿—åˆ†æ
  â”œâ”€ æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
  â””â”€ å‘Šè­¦è§„åˆ™æ£€æŸ¥
        â†“
   ç›®æ ‡ç³»ç»Ÿ
  â”œâ”€ Elasticsearchï¼ˆæ—¥å¿—æ£€ç´¢ï¼‰
  â”œâ”€ InfluxDBï¼ˆæŒ‡æ ‡å­˜å‚¨ï¼‰
  â””â”€ AlertManagerï¼ˆå‘Šè­¦ï¼‰
```

**Topic è®¾è®¡ï¼š**
```json
{
  "name": "app-logs",
  "partitions": 20,  // æŒ‰æœåŠ¡æ•°é‡è®¾ç½®
  "replication-factor": 3,
  "retention": "7d",  // 7å¤©çƒ­æ•°æ®
  "compression": "snappy"
}
```

**æ—¥å¿—æ ¼å¼ï¼š**
```json
{
  "timestamp": "2024-01-26T10:00:00.123Z",
  "level": "ERROR",
  "service": "user-service",
  "trace_id": "abc123xyz",
  "message": "Database connection failed",
  "error_type": "DatabaseException",
  "response_time": 5000,
  "status_code": 500,
  "user_id": "user123",
  "request_path": "/api/users/123"
}
```

**å®æ—¶ç»Ÿè®¡è®¾è®¡ï¼š**
```python
# ä½¿ç”¨ Kafka Streams å®æ—¶ç»Ÿè®¡
from kafka import KafkaConsumer, KafkaProducer
import json
from collections import defaultdict
import time

consumer = KafkaConsumer(
    'app-logs',
    group_id='log-aggregator',
    bootstrap_servers=['localhost:9092'],
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# æ»‘åŠ¨çª—å£ç»Ÿè®¡ï¼ˆ1åˆ†é’Ÿçª—å£ï¼‰
window_size = 60  # ç§’
metrics = defaultdict(lambda: {
    'error_count': 0,
    'total_count': 0,
    'response_times': [],
    'last_reset': time.time()
})

for message in consumer:
    log = message.value
    service = log['service']
    current_time = time.time()
    
    # é‡ç½®çª—å£
    if current_time - metrics[service]['last_reset'] > window_size:
        metrics[service] = {
            'error_count': 0,
            'total_count': 0,
            'response_times': [],
            'last_reset': current_time
        }
    
    # æ›´æ–°æŒ‡æ ‡
    metrics[service]['total_count'] += 1
    if log['level'] == 'ERROR':
        metrics[service]['error_count'] += 1
    if 'response_time' in log:
        metrics[service]['response_times'].append(log['response_time'])
    
    # è®¡ç®—ç»Ÿè®¡å€¼
    error_rate = metrics[service]['error_count'] / metrics[service]['total_count'] * 100
    avg_response_time = sum(metrics[service]['response_times']) / len(metrics[service]['response_times']) if metrics[service]['response_times'] else 0
    
    # å‘é€æŒ‡æ ‡åˆ°æŒ‡æ ‡ä¸»é¢˜
    producer.send('metrics-topic', {
        'service': service,
        'timestamp': current_time,
        'error_rate': error_rate,
        'avg_response_time': avg_response_time,
        'qps': metrics[service]['total_count'] / window_size
    })
    
    # å‘Šè­¦æ£€æŸ¥
    if error_rate > 5.0:  # é”™è¯¯ç‡è¶…è¿‡ 5%
        producer.send('alerts-topic', {
            'alert_type': 'high_error_rate',
            'service': service,
            'error_rate': error_rate,
            'timestamp': current_time
        })
```

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- **æ‰¹é‡å†™å…¥**ï¼šæ—¥å¿—æ”¶é›†å™¨æ‰¹é‡å†™å…¥ï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
- **å‹ç¼©**ï¼šä½¿ç”¨ Snappy å‹ç¼©ï¼Œå‡å°‘å­˜å‚¨ç©ºé—´
- **åˆ†åŒºç­–ç•¥**ï¼šæŒ‰æœåŠ¡ååˆ†åŒºï¼Œç›¸åŒæœåŠ¡çš„æ—¥å¿—åœ¨åŒä¸€åˆ†åŒº

**éªŒè¯æ•°æ®ï¼š**
- **ååé‡**ï¼š1000ä¸‡æ¡æ—¥å¿—/å°æ—¶ï¼ˆå•èŠ‚ç‚¹ï¼‰
- **å»¶è¿Ÿ**ï¼šç«¯åˆ°ç«¯å»¶è¿Ÿ < 2sï¼ˆåŒ…å«ç»Ÿè®¡ï¼‰
- **å­˜å‚¨**ï¼šæ¯å¤© 500GB æ—¥å¿—ï¼Œå‹ç¼©å 150GB

---

## ğŸ› å¸¸è§å‘ä¸æ’æŸ¥

### å‘1ï¼šæ¶ˆæ¯ä¸¢å¤±
**ç°è±¡**ï¼šProducer å‘é€æ¶ˆæ¯åï¼ŒConsumer æ¶ˆè´¹ä¸åˆ°
**åŸå› **ï¼š
1. `acks=0` æˆ– `acks=1`ï¼ŒLeader å´©æºƒåæ¶ˆæ¯ä¸¢å¤±
2. `enable.auto.commit=true`ï¼Œæ¶ˆè´¹å¤±è´¥ä½†å·²æäº¤åç§»é‡
3. å‰¯æœ¬æœªåŒæ­¥å®Œæˆï¼ŒLeader å´©æºƒ
**æ’æŸ¥**ï¼š
1. è®¾ç½® `acks=all`ï¼Œç­‰å¾…æ‰€æœ‰ ISR å‰¯æœ¬ç¡®è®¤
2. è®¾ç½® `min.insync.replicas=2`ï¼Œç¡®ä¿è‡³å°‘ 2 ä¸ªå‰¯æœ¬åŒæ­¥
3. è®¾ç½® `enable.auto.commit=false`ï¼Œæ‰‹åŠ¨æäº¤åç§»é‡
4. ç›‘æ§ `UnderReplicatedPartitions` æŒ‡æ ‡

### å‘2ï¼šæ¶ˆæ¯é‡å¤
**ç°è±¡**ï¼šConsumer é‡å¤æ¶ˆè´¹åŒä¸€æ¡æ¶ˆæ¯
**åŸå› **ï¼š
1. Producer é‡è¯•å¯¼è‡´æ¶ˆæ¯é‡å¤
2. Consumer æäº¤åç§»é‡å¤±è´¥ï¼Œé‡å¤æ‹‰å–
3. Rebalance å¯¼è‡´é‡å¤æ¶ˆè´¹
**æ’æŸ¥**ï¼š
1. å¯ç”¨å¹‚ç­‰æ€§ï¼š`enable.idempotence=true`
2. ä½¿ç”¨äº‹åŠ¡ï¼š`transactional.id` + `beginTransaction()` + `commitTransaction()`
3. å®ç°å¹‚ç­‰æ€§æ¶ˆè´¹ï¼šä½¿ç”¨æ•°æ®åº“å”¯ä¸€ç´¢å¼•æˆ– Redis å»é‡

### å‘3ï¼šæ¶ˆè´¹å»¶è¿Ÿï¼ˆLagï¼‰
**ç°è±¡**ï¼šConsumer Lag æŒç»­å¢é•¿
**åŸå› **ï¼š
1. Consumer å¤„ç†é€Ÿåº¦æ…¢
2. åˆ†åŒºæ•°ä¸è¶³ï¼Œæ— æ³•å¹¶è¡Œæ¶ˆè´¹
3. ç½‘ç»œå»¶è¿Ÿæˆ– Broker è´Ÿè½½é«˜
**æ’æŸ¥**ï¼š
1. ç›‘æ§ `records-lag-max` æŒ‡æ ‡
2. å¢åŠ  Consumer å®ä¾‹æ•°ï¼ˆä¸è¶…è¿‡åˆ†åŒºæ•°ï¼‰
3. ä¼˜åŒ– Consumer å¤„ç†é€»è¾‘
4. å¢åŠ åˆ†åŒºæ•°æé«˜å¹¶è¡Œåº¦

### å‘4ï¼šé¢‘ç¹ Rebalance
**ç°è±¡**ï¼šConsumer é¢‘ç¹ Rebalanceï¼Œæ¶ˆè´¹ä¸­æ–­
**åŸå› **ï¼š
1. `session.timeout.ms` è®¾ç½®è¿‡çŸ­
2. `max.poll.interval.ms` è®¾ç½®è¿‡çŸ­
3. æ¶ˆè´¹è€…å¤„ç†æ—¶é—´è¿‡é•¿
**æ’æŸ¥**ï¼š
1. å¢åŠ  `session.timeout.ms`ï¼ˆé»˜è®¤ 10sï¼Œå¯è®¾ç½® 30sï¼‰
2. å¢åŠ  `max.poll.interval.ms`ï¼ˆé»˜è®¤ 5minï¼Œå¯è®¾ç½® 10minï¼‰
3. ä¼˜åŒ– Consumer å¤„ç†é€»è¾‘ï¼Œå‡å°‘å¤„ç†æ—¶é—´
4. ä½¿ç”¨æ‰¹é‡å¤„ç†ï¼Œæé«˜å¤„ç†æ•ˆç‡

---

## éªŒè¯æ•°æ®

### Kafka æ€§èƒ½æµ‹è¯•

| åœºæ™¯ | ååé‡ | å»¶è¿Ÿ | è¯´æ˜ |
|-----|--------|------|------|
| Producerï¼ˆå•åˆ†åŒºï¼‰ | 100ä¸‡ msg/s | <1ms | å•æœºï¼Œé¡ºåºå†™ |
| Consumerï¼ˆå•æ¶ˆè´¹è€…ï¼‰ | 50ä¸‡ msg/s | <10ms | å•æœºï¼Œé¡ºåºè¯» |
| å¤šåˆ†åŒºï¼ˆ10åˆ†åŒºï¼‰ | 500ä¸‡ msg/s | <5ms | å¹¶è¡Œå¤„ç† |
| å‹ç¼©ï¼ˆSnappyï¼‰ | 200ä¸‡ msg/s | <2ms | å‹ç¼©åååé‡æå‡ |

### å­˜å‚¨æ€§èƒ½

```
å†™å…¥é€Ÿåº¦ï¼š100MB/sï¼ˆå•åˆ†åŒºï¼‰
è¯»å–é€Ÿåº¦ï¼š200MB/sï¼ˆå•åˆ†åŒºï¼‰
å‹ç¼©æ¯”ï¼š3:1ï¼ˆä½¿ç”¨ Snappyï¼‰
é›¶æ‹·è´æå‡ï¼š2-3å€ååé‡
```

### é›†ç¾¤æ€§èƒ½

| åœºæ™¯ | Broker æ•° | åˆ†åŒºæ•° | ååé‡ | P99 å»¶è¿Ÿ |
|-----|-----------|--------|--------|---------|
| å• Broker | 1 | 10 | 100ä¸‡ msg/s | 10ms |
| 3 Broker é›†ç¾¤ | 3 | 30 | 300ä¸‡ msg/s | 5ms |
| 5 Broker é›†ç¾¤ | 5 | 50 | 500ä¸‡ msg/s | 3ms |

---

## æ€»ç»“

1. **é«˜æ€§èƒ½åŸç†**
   - é¡ºåºå†™ç£ç›˜ï¼ˆå……åˆ†åˆ©ç”¨ç£ç›˜å¸¦å®½ï¼‰
   - åˆ†æ®µå­˜å‚¨ï¼ˆä¾¿äºç®¡ç†å’ŒæŸ¥æ‰¾ï¼‰
   - é›¶æ‹·è´ï¼ˆå‡å°‘ CPU æ‹·è´å’Œä¸Šä¸‹æ–‡åˆ‡æ¢ï¼‰
   - ISR å‰¯æœ¬æœºåˆ¶ï¼ˆå¹³è¡¡å¯é æ€§å’Œæ€§èƒ½ï¼‰

2. **æ•°æ®ç»“æ„ç»„åˆ**
   - åˆ†åŒº + Key è·¯ç”±ï¼šä¿è¯é¡ºåºæ¶ˆæ¯
   - æ‰¹æ¬¡ç´¯ç§¯ + å‹ç¼©ï¼šæé«˜ååé‡
   - åç§»é‡ç®¡ç† + å¹‚ç­‰æ€§ï¼šä¿è¯ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰

3. **é«˜çº§åº”ç”¨åœºæ™¯**
   - è®¢å•ç³»ç»Ÿï¼šäº‹ä»¶é©±åŠ¨æ¶æ„ï¼Œæœ€ç»ˆä¸€è‡´æ€§
   - æ•°æ®ç®¡é“ï¼šETL æµç¨‹ï¼Œå®æ—¶æ•°æ®å¤„ç†
   - æ—¥å¿—èšåˆï¼šæ—¥å¿—æ”¶é›†ã€ç»Ÿè®¡ã€å‘Šè­¦

4. **æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒ**
   - åˆç†è®¾ç½®åˆ†åŒºæ•°ï¼ˆå•åˆ†åŒº 20-50GBï¼‰
   - æ‰¹é‡å‘é€å’Œæ¶ˆè´¹ï¼ˆå‡å°‘ç½‘ç»œå¾€è¿”ï¼‰
   - ä½¿ç”¨å‹ç¼©ï¼ˆå‡å°‘ç½‘ç»œä¼ è¾“å’Œå­˜å‚¨ï¼‰
   - ç›‘æ§ Consumer Lagï¼ˆåŠæ—¶å‘ç°é—®é¢˜ï¼‰
